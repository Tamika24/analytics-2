#| echo: true
calc_Ci <- function(y_true, y_pred) {
# y_true is assumed to be a scalar (either 0 or 1)
if (y_true == 1) {
return(-log(y_pred))
} else {
return(0)
}
}
#| echo: true
g <- function(Yhat, Y, eps = 1e-15) {
# Yhat, Y : N × q matrices   (rows = obs, cols = classes)
N <- nrow(Y)
-sum( Y * log( pmax(Yhat, eps) ) ) / N
}
#| echo: true
af_forward <- function(X, Y, theta, m, nu)
{
N <- nrow(X)
p <- ncol(X)
q <- ncol(Y)
index <- 1:(2(p^2)) #W1 : p(p+p)
W1 <- matrix(theta[index], nrow=p)
index <- max(index)+1:(2*p) #b1 : (p+p)
b1 <- theta[index]
index <- max(index)+1:((2*p)*m) #W2 : (p+p)*m
W2 <- matrix(theta[index], nrow=2*p)
index <- max(index)+1:m #b2 : m
b2 <- theta[index]
index <- max(index)+1:(m*m) #W3 : (m*m)
W3 <- matrix(theta[index], nrow=m)
index <- max(index)+1:m #b3 : m
b3 <- theta[index]
index <- max(index)+1:(m*q) #W4 : (m*q)
W4 <- matrix(theta[index], nrow=m)
index <- max(index)+1:q #b4 : q
b4 <- theta[index]
#softmax function
softmax <- function(Z)
{
Z_shift <- Z - matrix(apply(Z, 2, max),          # subtract column-wise maxima
nrow = 3, ncol = ncol(Z),
byrow = TRUE)
expZ    <- exp(Z_shift)                          # exponentiate
denom   <- matrix(colSums(expZ),                 # column-wise sums
nrow = 3, ncol = ncol(Z),
byrow = TRUE)
expZ / denom                                     # element-wise division
}
#forward propagation
H1 <- tanh( X  %*% W1 + matrix(b1, N, 2*p, TRUE) )       # aug-layer
H2 <- tanh( H1 %*% W2 + matrix(b2, N, m, TRUE) )      # 2nd hidden
H3 <- tanh(H2 %*% W3 + matrix(b3, N, m, TRUE))
Z <- H3 %*% W4 + matrix(b4, N, q, TRUE)          # logits
probs <- softmax(Z)                                          # softmax
#losses & objective
loss <- g(probs, Y)                 # cross-entropy
obj  <- loss + (nu / 2) * sum(theta^2)
list(probs = probs, loss = loss, obj = obj)
}
for (i in seq_len(n_nu)) {
nu      <- nu_seq[i]
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# unpenalized loss on validation
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
#Optimize and record validation loss
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu      <- nu_seq[i]
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# unpenalized loss on validation
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
#| echo: true
af_forward <- function(X, Y, theta, m, nu)
{
N <- nrow(X)
p <- ncol(X)
q <- ncol(Y)
index <- 1:(2*(p^2)) #W1 : p(p+p)
W1 <- matrix(theta[index], nrow=p)
index <- max(index)+1:(2*p) #b1 : (p+p)
b1 <- theta[index]
index <- max(index)+1:((2*p)*m) #W2 : (p+p)*m
W2 <- matrix(theta[index], nrow=2*p)
index <- max(index)+1:m #b2 : m
b2 <- theta[index]
index <- max(index)+1:(m*m) #W3 : (m*m)
W3 <- matrix(theta[index], nrow=m)
index <- max(index)+1:m #b3 : m
b3 <- theta[index]
index <- max(index)+1:(m*q) #W4 : (m*q)
W4 <- matrix(theta[index], nrow=m)
index <- max(index)+1:q #b4 : q
b4 <- theta[index]
#softmax function
softmax <- function(Z)
{
Z_shift <- Z - matrix(apply(Z, 2, max),          # subtract column-wise maxima
nrow = 3, ncol = ncol(Z),
byrow = TRUE)
expZ    <- exp(Z_shift)                          # exponentiate
denom   <- matrix(colSums(expZ),                 # column-wise sums
nrow = 3, ncol = ncol(Z),
byrow = TRUE)
expZ / denom                                     # element-wise division
}
#forward propagation
H1 <- tanh( X  %*% W1 + matrix(b1, N, 2*p, TRUE) )       # aug-layer
H2 <- tanh( H1 %*% W2 + matrix(b2, N, m, TRUE) )      # 2nd hidden
H3 <- tanh(H2 %*% W3 + matrix(b3, N, m, TRUE))
Z <- H3 %*% W4 + matrix(b4, N, q, TRUE)          # logits
probs <- softmax(Z)                                          # softmax
#losses & objective
loss <- g(probs, Y)                 # cross-entropy
obj  <- loss + (nu / 2) * sum(theta^2)
list(probs = probs, loss = loss, obj = obj)
}
#prepare data
X <- as.matrix(dat[, c("X1", "X2", "X3")])
Y <- as.matrix(dat[, c("Y1", "Y2", "Y3")])
#Split into 80% training / 20% validation
set.seed(2025)
N     <- dim(X)[1]
set    <- sample(1:N, 0.8 * N, FALSE)
X_train <- X[set, , drop = FALSE]
Y_train <- Y[set, , drop = FALSE]
X_val   <- X[-set, , drop = FALSE]
Y_val   <- Y[-set, , drop = FALSE]
#Network dims & parameter count
m     <- 4
p     <- ncol(X_train)
q     <- ncol(Y_train)
npar  <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
#Penalised training objective (nu is global)
nu      <- 0.01
obj_pen <- function(pars) {
af_forward(X_train, Y_train, pars, m, nu)$obj
}
#Initialize random theta
theta_rand <- runif(npar, -1, 1)
#Sequence of regularisation levels
n_nu   <- 10
nu_seq <- exp(seq(-6, 2, length = n_nu))
#Optimize and record validation loss
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu      <- nu_seq[i]
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# unpenalized loss on validation
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
#| echo: true
af_forward <- function(X, Y, theta, m, nu)
{
N <- nrow(X)
p <- ncol(X)
q <- ncol(Y)
index <- 1:(2*(p^2)) #W1 : p(p+p)
W1 <- matrix(theta[index], nrow=p)
index <- max(index)+1:(2*p) #b1 : (p+p)
b1 <- theta[index]
index <- max(index)+1:((2*p)*m) #W2 : (p+p)*m
W2 <- matrix(theta[index], nrow=2*p)
index <- max(index)+1:m #b2 : m
b2 <- theta[index]
index <- max(index)+1:(m*m) #W3 : (m*m)
W3 <- matrix(theta[index], nrow=m)
index <- max(index)+1:m #b3 : m
b3 <- theta[index]
index <- max(index)+1:(m*q) #W4 : (m*q)
W4 <- matrix(theta[index], nrow=m)
index <- max(index)+1:q #b4 : q
b4 <- theta[index]
#softmax function but changed to row wise because of conformability?
softmax <- function(Z) {
Zs   <- Z - matrix(apply(Z, 1, max), nrow = nrow(Z), ncol = ncol(Z), byrow = FALSE)
expZ <- exp(Zs)
expZ / matrix(rowSums(expZ), nrow = nrow(Z), ncol = ncol(Z), byrow = FALSE)
}
#forward propagation
H1 <- tanh( X  %*% W1 + matrix(b1, N, 2*p, TRUE) )       # aug-layer
H2 <- tanh( H1 %*% W2 + matrix(b2, N, m, TRUE) )      # 2nd hidden
H3 <- tanh(H2 %*% W3 + matrix(b3, N, m, TRUE))
Z <- H3 %*% W4 + matrix(b4, N, q, TRUE)          # logits
probs <- softmax(Z)                                          # softmax
#losses & objective
loss <- g(probs, Y)                 # cross-entropy
obj  <- loss + (nu / 2) * sum(theta^2)
list(probs = probs, loss = loss, obj = obj)
}
#prepare data
X <- as.matrix(dat[, c("X1", "X2", "X3")])
Y <- as.matrix(dat[, c("Y1", "Y2", "Y3")])
#Split into 80% training / 20% validation
set.seed(2025)
N     <- dim(X)[1]
set    <- sample(1:N, 0.8 * N, FALSE)
X_train <- X[set, , drop = FALSE]
Y_train <- Y[set, , drop = FALSE]
X_val   <- X[-set, , drop = FALSE]
Y_val   <- Y[-set, , drop = FALSE]
#Network dims & parameter count
m     <- 4
p     <- ncol(X_train)
q     <- ncol(Y_train)
npar  <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
#Penalised training objective (nu is global)
nu      <- 0.01
obj_pen <- function(pars) {
af_forward(X_train, Y_train, pars, m, nu)$obj
}
#Initialize random theta
theta_rand <- runif(npar, -1, 1)
#Sequence of regularisation levels
n_nu   <- 10
nu_seq <- exp(seq(-6, 2, length = n_nu))
#Optimize and record validation loss
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu      <- nu_seq[i]
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# unpenalized loss on validation
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
# 7. Plot validation curve
plot(nu_seq, Val_error, type="b", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
best_nu <- nu_seq[best_i]
points(best_nu, Val_error[best_i], col="red", pch=16)
# 7. Plot validation curve
plot(nu_seq, Val_error, type="b", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
plot(nu_seq, Val_error, type="b", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
best_nu <- nu_seq[best_i]
points(best_nu, Val_error[best_i], col="red", pch=16)
legend("topright",
legend=paste("Chosen ν =", signif(best_nu, 3)),
col="red", pch=16, bty="n")
#Sequence of regularisation levels
n_nu   <- 20
nu_seq <- exp(seq(-6, 2, length = n_nu))
#Optimize and record validation loss
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu      <- nu_seq[i]
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# unpenalized loss on validation
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
for (i in seq_len(n_nu)) {
nu      <- nu_seq[i]
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# unpenalized loss on validation
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
# 7. Plot validation curve
plot(nu_seq, Val_error, type="b", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
# 7. Plot validation curve
plot(nu_seq, Val_error, type="b", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
best_nu <- nu_seq[best_i]
points(best_nu, Val_error[best_i], col="red", pch=16)
legend("topright",
legend=paste("Chosen ν =", signif(best_nu, 3)),
col="red", pch=16, bty="n")
#Sequence of regularisation levels
n_nu   <- 15
#Optimize and record validation loss
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu      <- nu_seq[i]
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# unpenalized loss on validation
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
# 7. Plot validation curve
plot(Val_error~ nu_seq, type="b", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
plot(Val_error~nu_seq, type="b")
# 7. Plot validation curve
plot(nu_seq, Val_error, type="b", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
#Sequence of regularisation levels
n_nu   <- 15
nu_seq <- exp(seq(-8, -4, length = n_nu))
#Optimize and record validation loss
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu      <- nu_seq[i]
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# unpenalized loss on validation
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
# 7. Plot validation curve
plot(Val_error~nu_seq,type = 'b')
plot(nu_seq, Val_error, type="b", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
plot(nu_seq, Val_error, type="b", pch=16, log="x",
xlab = expression(nu), ylab = "Validation Loss",
main = "Validation Loss vs Regularization (m = 4)")
abline(v = nu_seq[best_i], col = "red", lty = 2)
legend("topright",
legend = paste("min at ν =", signif(nu_seq[best_i],3)),
col = "red", lty = 2, bty="n")
plot(nu_seq, Val_error, type="b", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
best_nu <- nu_seq[best_i]
points(best_nu, Val_error[best_i], col="red", pch=16)
legend("topright",
legend=paste("Chosen ν =", signif(best_nu, 3)),
col="red", pch=16, bty="n")
plot(nu_seq, Val_error, type="b", pch=16, log="x",
xlab = expression(nu), ylab = "Validation Loss",
main = "Validation Loss vs Regularization (m = 4)")
abline(v = nu_seq[best_i], col = "red", lty = 2)
legend("topright",
legend = paste("min at ν =", signif(nu_seq[best_i],3)),
col = "red", lty = 2, bty="n")
focus <- nu_seq < 0.05
plot(nu_seq[focus], Val_error[focus], type="b", log="x",
xlab = expression(nu), ylab = "Validation Loss")
#Sequence of regularisation levels
n_nu   <- 20
nu_seq <- exp(seq(-10, -2, length = n_nu))
#Optimize and record validation loss
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu      <- nu_seq[i]
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# unpenalized loss on validation
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
#| echo: true
af_forward <- function(X, Y, theta, m, nu)
{
N <- nrow(X)
p <- ncol(X)
q <- ncol(Y)
index <- 1:(2*(p^2)) #W1 : p(p+p)
W1 <- matrix(theta[index], nrow=p)
index <- max(index)+1:(2*p) #b1 : (p+p)
b1 <- theta[index]
index <- max(index)+1:((2*p)*m) #W2 : (p+p)*m
W2 <- matrix(theta[index], nrow=2*p)
index <- max(index)+1:m #b2 : m
b2 <- theta[index]
index <- max(index)+1:(m*m) #W3 : (m*m)
W3 <- matrix(theta[index], nrow=m)
index <- max(index)+1:m #b3 : m
b3 <- theta[index]
index <- max(index)+1:(m*q) #W4 : (m*q)
W4 <- matrix(theta[index], nrow=m)
index <- max(index)+1:q #b4 : q
b4 <- theta[index]
#softmax function but changed to row wise because of conformability?
# softmax <- function(Z) {
#Zs   <- Z - matrix(apply(Z, 1, max), nrow = nrow(Z), ncol = ncol(Z), byrow = FALSE)
#expZ <- exp(Zs)
#expZ / matrix(rowSums(expZ), nrow = nrow(Z), ncol = ncol(Z), byrow = FALSE)
#}
softmax <- function(Z)
{
Z_shift <- Z - matrix(apply(Z, 2, max), nrow = 3, ncol = ncol(Z),byrow = TRUE)
expZ    <- exp(Z_shift)                          # exponentiate
denom   <- matrix(colSums(expZ),                 # column-wise sums
nrow = 3, ncol = ncol(Z),
byrow = TRUE)
expZ / denom
}
#forward propagation
H1 <- tanh( X  %*% W1 + matrix(b1, N, 2*p, TRUE) )       # aug-layer
H2 <- tanh( H1 %*% W2 + matrix(b2, N, m, TRUE) )      # 2nd hidden
H3 <- tanh(H2 %*% W3 + matrix(b3, N, m, TRUE))
Z <- H3 %*% W4 + matrix(b4, N, q, TRUE) # logits
P_3byN <- softmax(t(Z))   # t(Z) is q×N but q=3 here
probs   <- t(P_3byN)
#probs <- softmax(Z) didnt work because of conformable matrices
#losses & objective
loss <- g(probs, Y)                 # cross-entropy
obj  <- loss + (nu / 2) * sum(theta^2)
list(probs = probs, loss = loss, obj = obj)
}
#prepare data
X <- as.matrix(dat[, c("X1", "X2", "X3")])
Y <- as.matrix(dat[, c("Y1", "Y2", "Y3")])
#Split into 80% training / 20% validation
set.seed(2025)
N     <- dim(X)[1]
set    <- sample(1:N, 0.8 * N, FALSE)
X_train <- X[set, , drop = FALSE]
Y_train <- Y[set, , drop = FALSE]
X_val   <- X[-set, , drop = FALSE]
Y_val   <- Y[-set, , drop = FALSE]
#Network dims & parameter count
m     <- 4
p     <- ncol(X_train)
q     <- ncol(Y_train)
npar  <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
#Penalised training objective (nu is global)
nu      <- 0.01
obj_pen <- function(pars) {
af_forward(X_train, Y_train, pars, m, nu)$obj
}
#Initialize random theta
theta_rand <- runif(npar, -1, 1)
#Sequence of regularisation levels
n_nu   <- 20
nu_seq <- exp(seq(-10, -2, length = n_nu))
#Optimize and record validation loss
Val_error <- numeric(n_nu)
#Sequence of regularisation levels
n_nu   <- 10
nu_seq <- exp(seq(-10, -2, length = n_nu))
#Optimize and record validation loss
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu      <- nu_seq[i]
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# unpenalized loss on validation
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
# 7. Plot validation curve
plot(nu_seq, Val_error, type="b", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
#logged x to should more of the u shape, not sure though if it's right or which one to use
plot(nu_seq, Val_error, type="b", pch=16, log="x",
xlab = expression(nu), ylab = "Validation Loss",
main = "Validation Loss vs Regularization (m = 4)")
abline(v = nu_seq[best_i], col = "red", lty = 2)
legend("topright",
legend = paste("min at ν =", signif(nu_seq[best_i],3)),
col = "red", lty = 2, bty="n")
plot(nu_seq, Val_error, type="b", pch=16, log="x",
xlab = expression(nu), ylab = "Validation Loss",
main = "Validation Loss vs Regularization (m = 4)")
abline(v = nu_seq[best_i], col = "red", lty = 2)
legend("topright",
legend = paste("min at ν =", signif(nu_seq[best_i],3)),
col = "red", lty = 2, bty="n")
plot(nu_seq, Val_error, type="b", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
best_nu <- nu_seq[best_i]
points(best_nu, Val_error[best_i], col="red", pch=16)
legend("topright",
legend=paste("Chosen ν =", signif(best_nu, 3)),
col="red", pch=16, bty="n")
plot(nu_seq, Val_error, type="l", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
best_nu <- nu_seq[best_i]
points(best_nu, Val_error[best_i], col="red", pch=16)
legend("topright",
legend=paste("Chosen ν =", signif(best_nu, 3)),
col="red", pch=16, bty="n")
plot(nu_seq, Val_error, type="l", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
best_nu <- nu_seq[best_i]
points(best_nu, Val_error[best_i], col="red", pch=16)
legend("topright",
legend=paste("Chosen ν =", signif(best_nu, 3)),
col="red", pch=16, bty="n")
# 7. Plot validation curve
plot(nu_seq, Val_error, type="p", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
# 7. Plot validation curve
plot(nu_seq, Val_error, type="b", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
plot(nu_seq, Val_error, type="b", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
best_nu <- nu_seq[best_i]
points(best_nu, Val_error[best_i], col="red", pch=16)
legend("topright",
legend=paste("Chosen ν =", signif(best_nu, 3)),
col="red", pch=16, bty="n")

<<<<<<< HEAD
index <- max(index)+1:m #b2 : m
b2 <- theta[index]
index <- max(index)+1:(m*m) #W3 : (m*m)
W3 <- matrix(theta[index], nrow=m)
index <- max(index)+1:m #b3 : m
b3 <- theta[index]
index <- max(index)+1:(m*q) #W4 : (m*q)
W4 <- matrix(theta[index], nrow=m)
index <- max(index)+1:q #b4 : q
b4 <- theta[index]
#softmax function but changed to row wise because of conformability?
# softmax <- function(Z) {
#Zs   <- Z - matrix(apply(Z, 1, max), nrow = nrow(Z), ncol = ncol(Z), byrow = FALSE)
#expZ <- exp(Zs)
#expZ / matrix(rowSums(expZ), nrow = nrow(Z), ncol = ncol(Z), byrow = FALSE)
#}
# softmax <- function(Z)
#{
#Z_shift <- Z - matrix(apply(Z, 2, max), nrow = 3, ncol = ncol(Z),byrow = TRUE)
#expZ    <- exp(Z_shift)                          # exponentiate
#denom   <- matrix(colSums(expZ),                 # column-wise sums
#                 nrow = 3, ncol = ncol(Z),
#                byrow = TRUE)
#expZ / denom
#}
#forward propagation
H1 <- tanh( X  %*% W1 + matrix(b1, N, 2*p, TRUE) )       # aug-layer
H2 <- tanh( H1 %*% W2 + matrix(b2, N, m, TRUE) )      # 2nd hidden
H3 <- tanh(H2 %*% W3 + matrix(b3, N, m, TRUE))
Z <- H3 %*% W4 + matrix(b4, N, q, TRUE) # logits
#used the colsums in softmax but then transposed these probabilities
P_3byN <- softmax(t(Z))   # t(Z) is q×N but q=3 here
probs   <- t(P_3byN)
#probs <- softmax(Z) didnt work because of un-conformable matrices when i used the colsums in softmax and didn't transpose anything
#losses & objective
loss <- g(probs, Y)                 # cross-entropy
obj  <- loss + (nu / 2) * sum(theta^2)
list(probs = probs, loss = loss, obj = obj)
}
(p + 1)m + (m + 1)m + (m + 1)q.
set.seed(2025)
n <- nrow(dat)
train_idx <- sample(1:n, size = 0.8 * n)
dat_train <- dat[train_idx,]
dat_valid <- dat[-train_idx,]
# Prepare input/output
X_train <- as.matrix(dat_train[,1:3])
Y_train <- as.matrix(dat_train[,4:6])
X_valid <- as.matrix(dat_valid[,1:3])
Y_valid <- as.matrix(dat_valid[,4:6])
# Grid of nu values
nu_vals <- exp(seq(-6, 2, length = 50))
val_losses <- numeric(length(nu_vals))
for (i in seq_along(nu_vals)) {
# Random initialisation
params <- list(
W1 = matrix(rnorm((3+1)*4, sd=0.1), nrow=4),
W2 = matrix(rnorm((4+1)*4, sd=0.1), nrow=4),
W3 = matrix(rnorm((4+1)*3, sd=0.1), nrow=3)
)
out <- forward_AFnetwork(X_valid, Y_valid, params, nu_vals[i])
val_losses[i] <- out$loss
}
# Set seed and split data
set.seed(2025)
N <- nrow(X)
af_forward <- function(X, Y, theta, m, nu)
=======
# Reinitialize random parameters
theta_rand <- runif(npars, -1, 1)
res_opt_best <- nlm(obj_pen_best, p = theta_rand, iterlim = 1000)
theta_best <- res_opt_best$estimate
# Plot response curves by varying X1 and X2 separately
# Helper function to predict probability curves
predict_curve <- function(var_seq, varname, fixed_X2 = 0, fixed_X3 = 0, pars, m)
{
n <- length(var_seq)
input <- matrix(0, nrow = n, ncol = 3)
colnames(input) <- c("X1", "X2", "X3")
input[, "X1"] <- if (varname == "X1") var_seq else fixed_X2
input[, "X2"] <- if (varname == "X2") var_seq else fixed_X2
input[, "X3"] <- fixed_X3
q <- 3
preds <- af_forward(input, Y = matrix(0, nrow=n, ncol=q), pars, m, v=0.                      )$probs
out <- as.data.frame(preds)
colnames(out) <- c("alpha", "beta", "rho")
out[[varname]] <- var_seq
return(out)
}
# Create sequences
X_seq <- seq(-4, 4, length.out = 100)
# Response curves for Detector Type A (X3=1) and Type B (X3=0)
curve_X1_A <- predict_curve(X_seq, "X1", fixed_X2=0, fixed_X3=1, theta_best, m)
#| echo: false
dat = read.table("Collider_Data_2025.txt", h = TRUE,stringsAsFactors =TRUE) # Text columns automatically become factors (categories)
# Load libraries
library(ggplot2)
library(dplyr)
library(r2symbols)
library(tidyr)
library(formatR)
nu <- best_nu
install.packages("formatR")
#| echo: false
dat = read.table("Collider_Data_2025.txt", h = TRUE,stringsAsFactors =TRUE) # Text columns automatically become factors (categories)
# Load libraries
library(ggplot2)
library(dplyr)
library(r2symbols)
library(tidyr)
library(formatR)
#| echo: false
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
#| echo: true
#| label: fig-scatter
#| fig-cap: "This is a scatter plot of the Collider data in the X1 and X2 feature space, with points colour-coded by true particle class (α in red, β in blue, ρ in green), illustrating non-linear boundaries that motivate using a neural network for classification."
dat$response <- apply(dat[, c("Y1", "Y2", "Y3")], 1, function (x){
if (x[1] == 1) return("code-Alpha")
if (x[2] == 1) return("code-Beta")
if (x[3] == 1) return("code-Rho")
})
ggplot(dat, aes(x=X1, y=X2, color=response))+
geom_point(size=2)+
coord_fixed()+ # 1:1 aspect ratio
labs(title = "Scatterplot of particles in feature space",
x = "First coordinate (X1)", y = "Second coordinate (X2)",
color = "Particle type")+
theme_minimal()
#| echo: true
softmax <- function(Z)
{
Z_shift <- Z - matrix(apply(Z, 2, max), nrow = 3, ncol = ncol(Z),
byrow = TRUE)
#Subtract the column max for numerical stability (to avoid computational overflow when exponentiating)
expZ    <- exp(Z_shift)
denom   <- matrix(colSums(expZ),
nrow = 3, ncol = ncol(Z), byrow =TRUE)
# column-wise sums...sum across 3 classes
# convert to matrix for conformability
expZ / denom
}
#| echo: true
g <- function(Yhat, Y, eps = 1e-15) {
# Yhat, Y : N × q matrices   (rows = observations, columns = classes)
N <- nrow(Y)
-sum( Y * log( pmax(Yhat, eps) ) ) / N
# pmax() replaces any element of Yhat that is smaller than eps with eps   #ensures no value passed to log() is <= zero
}
#| echo: true
# X: input matrix (N x p)
# Y: output matrix (N x q)
# theta: parameter vector with all weights and biases
# m: number of nodes on hidden layer
# v: regularisation parameter
af_forward <- function(X, Y, theta, m, v)
>>>>>>> e5f2842f7ebe83064a019aaef75969ca4fb49224
{
N <- nrow(X)
p <- ncol(X)
q <- ncol(Y)
<<<<<<< HEAD
=======
# Populate weight-matrix and bias vectors by unpacking theta:
>>>>>>> e5f2842f7ebe83064a019aaef75969ca4fb49224
index <- 1:(2*(p^2)) #W1 : p(p+p)
W1 <- matrix(theta[index], nrow=p)
index <- max(index)+1:(2*p) #b1 : (p+p)
b1 <- theta[index]
index <- max(index)+1:((2*p)*m) #W2 : (p+p)*m
W2 <- matrix(theta[index], nrow=2*p)
index <- max(index)+1:m #b2 : m
b2 <- theta[index]
index <- max(index)+1:(m*m) #W3 : (m*m)
W3 <- matrix(theta[index], nrow=m)
index <- max(index)+1:m #b3 : m
b3 <- theta[index]
index <- max(index)+1:(m*q) #W4 : (m*q)
W4 <- matrix(theta[index], nrow=m)
index <- max(index)+1:q #b4 : q
b4 <- theta[index]
<<<<<<< HEAD
#softmax function but changed to row wise because of conformability?
# softmax <- function(Z) {
#Zs   <- Z - matrix(apply(Z, 1, max), nrow = nrow(Z), ncol = ncol(Z), byrow = FALSE)
#expZ <- exp(Zs)
#expZ / matrix(rowSums(expZ), nrow = nrow(Z), ncol = ncol(Z), byrow = FALSE)
#}
# softmax <- function(Z)
#{
#Z_shift <- Z - matrix(apply(Z, 2, max), nrow = 3, ncol = ncol(Z),byrow = TRUE)
#expZ    <- exp(Z_shift)                          # exponentiate
#denom   <- matrix(colSums(expZ),                 # column-wise sums
#                 nrow = 3, ncol = ncol(Z),
#                byrow = TRUE)
#expZ / denom
#}
#forward propagation
H1 <- tanh( X  %*% W1 + matrix(b1, N, 2*p, TRUE) )       # aug-layer
H2 <- tanh( H1 %*% W2 + matrix(b2, N, m, TRUE) )      # 2nd hidden
H3 <- tanh(H2 %*% W3 + matrix(b3, N, m, TRUE))
Z <- H3 %*% W4 + matrix(b4, N, q, TRUE) # logits
#used the colsums in softmax but then transposed these probabilities
P_3byN <- softmax(t(Z))   # t(Z) is q×N but q=3 here
probs   <- t(P_3byN)
#probs <- softmax(Z) didnt work because of un-conformable matrices when i used the colsums in softmax and didn't transpose anything
#losses & objective
loss <- g(probs, Y)                 # cross-entropy
obj  <- loss + (nu / 2) * sum(theta^2)
list(probs = probs, loss = loss, obj = obj)
}
# Set seed and split data
set.seed(2025)
N <- nrow(X)
# Set seed and split data
set.seed(2025)
N <- nrow(dat)
set <- sample(1:N, size = 0.8 * N, replace = FALSE)
dat_train <- dat[set, , drop = FALSE]
dat_valid <- dat[-set, , drop = FALSE]
# Prepare input/output matrices
X_train <- as.matrix(dat_train[, c("X1", "X2", "X3")])
Y_train <- as.matrix(dat_train[, c("Y1", "Y2", "Y3")])
X_val   <- as.matrix(dat_valid[, c("X1", "X2", "X3")])
Y_val   <- as.matrix(dat_valid[, c("Y1", "Y2", "Y3")])
# Network dimensions and parameter count
m    <- 4
p    <- ncol(X_train)
q    <- ncol(Y_train)
npar <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
# Penalized training objective (nu will be set inside loop)
obj_pen <- function(theta) {
af_forward(X_train, Y_train, theta, m, nu)$obj
}
# Random initialization of parameters
theta_rand <- runif(npar, -1, 1)
# Sequence of regularization levels
n_nu   <- 20
nu_seq <- exp(seq(-10, -2, length = n_nu))
# Initialize validation loss vector
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu <- nu_seq[i]  # Set global nu
# Optimize penalized objective on training set
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# Evaluate unpenalized validation loss
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
# Set seed and split data
set.seed(2025)
N <- nrow(dat)
set <- sample(1:N, size = 0.8 * N, replace = FALSE)
dat_train <- dat[set, , drop = FALSE]
dat_valid <- dat[-set, , drop = FALSE]
# Prepare input/output matrices
X_train <- as.matrix(dat_train[, c("X1", "X2", "X3")])
Y_train <- as.matrix(dat_train[, c("Y1", "Y2", "Y3")])
X_val   <- as.matrix(dat_valid[, c("X1", "X2", "X3")])
Y_val   <- as.matrix(dat_valid[, c("Y1", "Y2", "Y3")])
# Network dimensions and parameter count
m    <- 4
p    <- ncol(X_train)
q    <- ncol(Y_train)
npar <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
# Penalized training objective (nu will be set inside loop)
obj_pen <- function(theta) {
af_forward(X_train, Y_train, theta, m, nu)$obj
}
# Random initialization of parameters
theta_rand <- runif(npar, -1, 1)
# Sequence of regularization levels
n_nu   <- 20
nu_seq <- exp(seq(-10, -2, length = n_nu))
# Initialize validation loss vector
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu <- nu_seq[i]  # Set global nu
# Optimize penalized objective on training set
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# Evaluate unpenalized validation loss
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
x
# Set seed and split data
set.seed(2025)
N <- nrow(dat)
set <- sample(1:N, size = 0.8 * N, replace = FALSE)
dat_train <- dat[set, , drop = FALSE]
dat_valid <- dat[-set, , drop = FALSE]
# Prepare input/output matrices
X_train <- as.matrix(dat_train[, c("X1", "X2", "X3")])
Y_train <- as.matrix(dat_train[, c("Y1", "Y2", "Y3")])
X_val   <- as.matrix(dat_valid[, c("X1", "X2", "X3")])
Y_val   <- as.matrix(dat_valid[, c("Y1", "Y2", "Y3")])
# Network dimensions and parameter count
m    <- 4
p    <- ncol(X_train)
q    <- ncol(Y_train)
npar <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
# Penalized training objective (nu will be set inside loop)
obj_pen <- function(theta) {
af_forward(X_train, Y_train, theta, m, nu)$obj
}
# Random initialization of parameters
theta_rand <- runif(npar, -1, 1)
# Sequence of regularization levels
n_nu   <- 20
nu_seq <- exp(seq(-10, -2, length = n_nu))
# Initialize validation loss vector
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu <- nu_seq[i]  # Set global nu
# Optimize penalized objective on training set
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# Evaluate unpenalized validation loss
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
# Set seed and split data
set.seed(2025)
N <- nrow(dat)
set <- sample(1:N, size = 0.8 * N, replace = FALSE)
dat_train <- dat[set, , drop = FALSE]
dat_valid <- dat[-set, , drop = FALSE]
# Prepare input/output matrices
X_train <- as.matrix(dat_train[, c("X1", "X2", "X3")])
Y_train <- as.matrix(dat_train[, c("Y1", "Y2", "Y3")])
X_val   <- as.matrix(dat_valid[, c("X1", "X2", "X3")])
Y_val   <- as.matrix(dat_valid[, c("Y1", "Y2", "Y3")])
# Network dimensions and parameter count
m    <- 4
p    <- ncol(X_train)
q    <- ncol(Y_train)
npar <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
# Penalized training objective (nu will be set inside loop)
obj_pen <- function(theta) {
af_forward(X_train, Y_train, theta, m, nu)$obj
}
# Random initialization of parameters
theta_rand <- runif(npar, -1, 1)
# Sequence of regularization levels
n_nu   <- 20
nu_seq <- exp(seq(-10, -2, length = n_nu))
# Initialize validation loss vector
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu <- nu_seq[i]  # Set global nu
# Optimize penalized objective on training set
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# Evaluate unpenalized validation loss
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
#prepare data
X <- as.matrix(dat[, c("X1", "X2", "X3")])
Y <- as.matrix(dat[, c("Y1", "Y2", "Y3")])
# Set seed and split data
set.seed(2025)
N <- nrow(X)
set <- sample(1:N, size = 0.8 * N)
X_train <- X[set, , drop = FALSE]
Y_train <- Y[set, , drop = FALSE]
X_val   <- X[-set, , drop = FALSE]
Y_val   <- Y[-set, , drop = FALSE]
# Network dimensions and parameter count
m    <- 4
p    <- ncol(X_train)
q    <- ncol(Y_train)
npar <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
# Penalized training objective (nu will be set inside loop)
obj_pen <- function(theta) {
af_forward(X_train, Y_train, theta, m, nu)$obj
}
# Random initialization of parameters
theta_rand <- runif(npar, -1, 1)
# Sequence of regularization levels
n_nu   <- 20
nu_seq <- exp(seq(-10, -2, length = n_nu))
# Initialize validation loss vector
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu <- nu_seq[i]  # Set global nu
# Optimize penalized objective on training set
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# Evaluate unpenalized validation loss
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
#logged x to should more of the u shape but seems to be plotting something not right ahah so maybe not this one
#plot(nu_seq, Val_error, type="b", pch=16, log="x",
#     xlab = expression(nu), ylab = "Validation Loss",
#    main = "Validation Loss vs Regularization (m = 4)")
#abline(v = nu_seq[best_i], col = "red", lty = 2)
#legend("topright",
#       legend = paste("min at ν =", signif(nu_seq[best_i],3)),
#      col = "red", lty = 2, bty="n")
# Set seed and split data
set.seed(2025)
N <- nrow(X)
set <- sample(1:N, size = 0.8 * N)
X_train <- X[set, , drop = FALSE]
Y_train <- Y[set, , drop = FALSE]
X_val   <- X[-set, , drop = FALSE]
Y_val   <- Y[-set, , drop = FALSE]
# Network dimensions and parameter count
m    <- 4
p    <- ncol(X_train)
q    <- ncol(Y_train)
npar <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
# Penalized training objective (nu will be set inside loop)
obj_pen <- function(theta) {
af_forward(X_train, Y_train, theta, m, nu)$obj
}
# Random initialization of parameters
theta_rand <- runif(npar, -1, 1)
# Sequence of regularization levels
n_nu   <- 20
nu_seq <- exp(seq(-10, -2, length = n_nu))
# Initialize validation loss vector
Val_error <- numeric(n_nu)
# Plot validation curve
plot(nu_seq, Val_error, type = "b", pch = 16,
xlab = expression(nu), ylab = "Validation Loss",
main = "Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
# Plot validation curve
plot(nu_seq, Val_error, type = "b", pch = 16,
xlab = expression(nu), ylab = "Validation Loss",
main = "Validation Loss vs Regularization (m = 4)")
points(best_nu, Val_error[best_i], col = "red", pch = 16)
best_i  <- which.min(Val_error)
best_nu <- nu_seq[best_i]
points(best_nu, Val_error[best_i], col = "red", pch = 16)
legend("topright",
legend = paste("Chosen ν =", signif(best_nu, 3)),
col = "red", pch = 16, bty = "n")
plot(nu_seq, Val_error, type = "b", pch = 16,
xlab = expression(nu), ylab = "Validation Loss",
main = "Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
best_nu <- nu_seq[best_i]
points(best_nu, Val_error[best_i], col = "red", pch = 16)
legend("topright",
legend = paste("Chosen ν =", signif(best_nu, 3)),
col = "red", pch = 16, bty = "n")
#Split into 80% training / 20% validation
set.seed(2025)
set    <- sample(1:N, 0.8 * N, FALSE)
X_train <- X[set, , drop = FALSE]
X_val   <- X[-set, , drop = FALSE]
#Network dims & parameter count
m     <- 4
q     <- ncol(Y_train)
#Penalised training objective (nu is global)
nu      <- 0.01
obj_pen <- function(pars) {
af_forward(X_train, Y_train, pars, m, nu)$obj
}
#Initialize random theta
theta_rand <- runif(npar, -1, 1)
#Sequence of regularisation levels
n_nu   <- 20
#expanded the grid to show more of the u shape
nu_seq <- exp(seq(-10, -2, length = n_nu))
#Optimize and record validation loss
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu      <- nu_seq[i]
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# unpenalized loss on validation
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
for (i in seq_len(n_nu)) {
nu      <- nu_seq[i]
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# unpenalized loss on validation
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
# 7. Plot validation curve
plot(nu_seq, Val_error, type="b", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
best_nu <- nu_seq[best_i]
points(best_nu, Val_error[best_i], col="red", pch=16)
legend("topright",
legend=paste("Chosen ν =", signif(best_nu, 3)),
col="red", pch=16, bty="n")
# 7. Plot validation curve
plot(nu_seq, Val_error, type="b", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
# 7. Plot validation curve
plot(nu_seq, Val_error, type="b", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
points(best_nu, Val_error[best_i], col="red", pch=16)
legend("topright",
legend=paste("Chosen ν =", signif(best_nu, 3)),
col="red", pch=16, bty="n")
plot(nu_seq, Val_error, type="b", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
best_nu <- nu_seq[best_i]
points(best_nu, Val_error[best_i], col="red", pch=16)
legend("topright",
legend=paste("Chosen ν =", signif(best_nu, 3)),
col="red", pch=16, bty="n")
# Set seed and split data
set.seed(2025)
set <- sample(1:N, size = 0.8 * N, replace = FALSE)
dat_valid <- dat[-set, , drop = FALSE]
Y_train <- as.matrix(dat_train[, c("Y1", "Y2", "Y3")])
Y_val   <- as.matrix(dat_valid[, c("Y1", "Y2", "Y3")])
p    <- ncol(X_train)
npar <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
# Sequence of regularization levels
n_nu   <- 20
nu_seq <- exp(seq(-10, -2, length = n_nu))
# Initialize validation loss vector
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu <- nu_seq[i]  # Set global nu
# NEW: Random reinitialization of parameters at each nu
theta_rand <- runif(npar, -1, 1)
# Optimize penalized objective on training set
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 2000)  # Increase iterlim
# Evaluate unpenalized validation loss
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
for (i in seq_len(n_nu)) {
nu <- nu_seq[i]  # Set global nu
# NEW: Random reinitialization of parameters at each nu
theta_rand <- runif(npar, -1, 1)
# Optimize penalized objective on training set
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 2000)  # Increase iterlim
# Evaluate unpenalized validation loss
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
# Plot validation curve
plot(log(nu_seq), Val_error, type = "b", pch = 16,
xlab = expression(log(nu)), ylab = "Validation Loss",
main = "Validation Loss vs Regularization (m = 4)")
for (i in seq_len(n_nu)) {
nu <- nu_seq[i]  # Set global nu
# NEW: Random reinitialization of parameters at each nu
theta_rand <- runif(npar, -1, 1)
# Optimize penalized objective on training set
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 2000)  # Increase iterlim
# Evaluate unpenalized validation loss
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
for (i in seq_len(n_nu)) {
nu <- nu_seq[i]  # Set global nu
# NEW: Random reinitialization of parameters at each nu
theta_rand <- runif(npar, -1, 1)
# Optimize penalized objective on training set
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 2000)  # Increase iterlim
# Evaluate unpenalized validation loss
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
# Set seed and split data
set.seed(2025)
N <- nrow(dat)
set <- sample(1:N, size = 0.8 * N, replace = FALSE)
dat_train <- dat[set, , drop = FALSE]
dat_valid <- dat[-set, , drop = FALSE]
# Prepare input/output matrices
X_train <- as.matrix(dat_train[, c("X1", "X2", "X3")])
Y_train <- as.matrix(dat_train[, c("Y1", "Y2", "Y3")])
X_val   <- as.matrix(dat_valid[, c("X1", "X2", "X3")])
Y_val   <- as.matrix(dat_valid[, c("Y1", "Y2", "Y3")])
# Network dimensions and parameter count
m    <- 4
p    <- ncol(X_train)
q    <- ncol(Y_train)
npar <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
# Penalized training objective (nu will be set inside loop)
obj_pen <- function(theta) {
af_forward(X_train, Y_train, theta, m, nu)$obj
}
# Sequence of regularization levels
n_nu   <- 20
nu_seq <- exp(seq(-10, -2, length = n_nu))
# Initialize validation loss vector
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu <- nu_seq[i]  # Set global nu
# NEW: Random but small initialization
theta_rand <- runif(npar, -0.5, 0.5)
# Optimize penalized objective
res_opt <- optim(theta_rand, obj_pen, method = "BFGS",
control = list(maxit = 500))   # Limit to 500 iterations max
# Evaluate unpenalized validation loss
Val_error[i] <- af_forward(X_val, Y_val, res_opt$par, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
=======
#forward propagation
H1 <- tanh( X  %*% W1 + matrix(b1, N, 2*p, TRUE) ) # aug-layer output
H2 <- tanh( H1 %*% W2 + matrix(b2, N, m, TRUE) ) # 1st hidden layer output
H3 <- tanh(H2 %*% W3 + matrix(b3, N, m, TRUE)) # 2nd hidden layer output
Z <- H3 %*% W4 + matrix(b4, N, q, TRUE) # final layer to get logits
# apply softmax across logits
P_3byN <- softmax(t(Z))
# temporarily transpose because softmax expects input where columns are different samples
probs   <- t(P_3byN)
#losses & objective
loss <- g(probs, Y) # cross-entropy
obj  <- loss + (v / 2) * sum(theta^2) # L2 regularisation
list(probs = probs, loss = loss, obj = obj)
}
#| echo: true
set.seed(2025)
# Step 1: Split the data into training and validation sets (80%/20%)
n <- nrow(dat)
train_size <- floor(0.8 * n)
train_indices <- sample(1:n, train_size)
train_data <- dat[train_indices, ]
valid_data <- dat[-train_indices, ]
# Step 2: Prepare the training and validation datasets
# Input features
X_train <- as.matrix(train_data[, 1:3])
# Response variables(one-hot encoded)
Y_train <- as.matrix(train_data[, 4:6])
# Input features
X_valid <- as.matrix(valid_data[, 1:3])
# Response variables(one-hot encoded)
Y_valid <- as.matrix(valid_data[, 4:6])
# Step 3: Define the objective function with regularization
objective_fn <- function(theta, X, Y, m, v) {
result <- af_forward(X, Y, theta, m, v)
return(result$obj)
}
# Step 4: Grid search over regularization parameter nu
v_values <- exp(seq(-6, 2, length.out = 15))
validation_errors <- numeric(length(v_values))
for (i in 1:length(v_values)) {
v <- v_values[i]
# Initial random theta
p <- ncol(X_train)
q <- ncol(Y_train)
m <- 4
npars  <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
theta_rand <- runif(npars, -1, 1)
# Fit the model using optim() to minimize the objective function
fit <- optim(theta_rand, objective_fn, X = X_train, Y = Y_train, m = 4,               v=v)
# Get the predicted probabilities for validation set
Yhat_valid <- af_forward(X_valid, Y_valid, fit$par, m = 4, v=v)$probs
# Compute the validation error
validation_errors[i] <- g(Yhat_valid, Y_valid)
}
# Step 5: Plot validation error vs nu
plot(v_values, validation_errors, type = "b", col = "blue", pch = 19,
asp = 1, log="x",
xlab = "Regularization Parameter (ν)", ylab = "Validation Error",
main = "Validation Error vs Regularization Parameter (ν)")
grid()
# Step 6: Choose the optimal regularization level (ν)
optimal_v <- v_values[which.min(validation_errors)]
#| echo: true
m <- 4
#best_nu <- exp(-2)
#nu <- best_nu
# Re-train model at best nu
obj_pen_best <- function(pars)
{
af_forward(X_train, Y_train, pars, m, nu)$obj
}
# Reinitialize random parameters
theta_rand <- runif(npars, -1, 1)
res_opt_best <- nlm(obj_pen_best, p = theta_rand, iterlim = 1000)
theta_best <- res_opt_best$estimate
# Plot response curves by varying X1 and X2 separately
# Helper function to predict probability curves
predict_curve <- function(var_seq, varname, fixed_X2 = 0, fixed_X3 = 0, pars, m)
{
n <- length(var_seq)
input <- matrix(0, nrow = n, ncol = 3)
colnames(input) <- c("X1", "X2", "X3")
input[, "X1"] <- if (varname == "X1") var_seq else fixed_X2
input[, "X2"] <- if (varname == "X2") var_seq else fixed_X2
input[, "X3"] <- fixed_X3
q <- 3
preds <- af_forward(input, Y = matrix(0, nrow=n, ncol=q), pars, m, v=0.                      )$probs
out <- as.data.frame(preds)
colnames(out) <- c("alpha", "beta", "rho")
out[[varname]] <- var_seq
return(out)
}
# Create sequences
X_seq <- seq(-4, 4, length.out = 100)
# Response curves for Detector Type A (X3=1) and Type B (X3=0)
curve_X1_A <- predict_curve(X_seq, "X1", fixed_X2=0, fixed_X3=1, theta_best, m)
curve_X1_B <- predict_curve(X_seq, "X1", fixed_X2=0, fixed_X3=0, theta_best, m)
curve_X2_A <- predict_curve(X_seq, "X2", fixed_X2=0, fixed_X3=1, theta_best, m)
curve_X2_B <- predict_curve(X_seq, "X2", fixed_X2=0, fixed_X3=0, theta_best, m)
# Helper to prepare data
prepare_plot_data <- function(curve_data, varname, type_label)
{
df <- as.data.frame(curve_data)
colnames(df) <- c(varname, "alpha", "beta", "rho")
df$Detector <- type_label
df <- pivot_longer(df, cols = c("alpha", "beta", "rho"),
names_to = "Class", values_to = "Probability")
return(df)
}
plot_data_X1 <- bind_rows(
prepare_plot_data(curve_X1_A, "X1", "Type A"),
prepare_plot_data(curve_X1_B, "X1", "Type B"))
ggplot(plot_data_X1, aes(x = X1, y = Probability, color = Class)) +
geom_line() +
facet_wrap(~ Detector) +
labs(title = "Predicted Class Probabilities vs X1",
x = "X1", y = "Probability") +
theme_minimal() +
theme(aspect.ratio = 1)
plot_data_X2 <- bind_rows(
prepare_plot_data(curve_X2_A, "X2", "Type A"),
prepare_plot_data(curve_X2_B, "X2", "Type B"))
ggplot(plot_data_X2, aes(x = X2, y = Probability, color = Class)) +
geom_line() +
facet_wrap(~ Detector) +
labs(title = "Predicted Class Probabilities vs X2",
x = "X2", y = "Probability") +
theme_minimal() +
theme(aspect.ratio = 1)
set.seed(2025)
m <- 4
# Step 1: Split the data into training and validation sets (80%/20%)
n <- nrow(dat)
train_size <- floor(0.8 * n)
train_indices <- sample(1:n, train_size)
train_data <- dat[train_indices, ]
valid_data <- dat[-train_indices, ]
# Step 2: Prepare the training and validation datasets
X_train <- as.matrix(train_data[, 1:3])  # Input features
Y_train <- as.matrix(train_data[, 4:6])  # Response variables (one-hot encoded)
X_valid <- as.matrix(valid_data[, 1:3])  # Input features
Y_valid <- as.matrix(valid_data[, 4:6])  # Response variables (one-hot encoded)
# Step 3: Define the objective function with regularization
v=0.01
obj_pen() <- function(pars) {
result <- af_forward(X_train, Y_train, theta, m, v)
return(result$obj)
}
theta_rand = runif(npars,-1,1)
obj_pen(theta_rand)
res_opt = nlm(obj_pen,theta_rand,iterlim = 1000)
res_opt
# Step 4: Grid search over regularization parameter nu
n_nu <- 15
validation_errors = rep(NA,n_nu)
v_values <- exp(seq(-6, 2, length.out = n_nu))
for (i in 1:n_nu) {
v <- v_values[i]
res_opt = nlm(obj_pen,theta_rand,iterlim = 1000)
res_val = af_forward(X_val,Y_val,res_opt$estimate,m,0)
validation_errors[i] = res_val$obj
}
plot(validation_errors~v_values,type = 'b')
# Initial random theta
p <- ncol(X_train)
q <- ncol(Y_train)
m <- 4
npars  <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
theta_rand <- runif(npars, -1, 1)
# Fit the model using optim() to minimize the objective function
fit <- optim(theta_rand, objective_fn, X = X_train, Y = Y_train, m = 4, v=v)
# Get the predicted probabilities for validation set
Yhat_valid <- af_forward(X_valid, Y_valid, fit$par, m = 4, v=v)$probs
# Compute the validation error
validation_errors[i] <- g(Yhat_valid, Y_valid)
}
for (i in 1:n_nu) {
v <- v_values[i]
res_opt = nlm(obj_pen,theta_rand,iterlim = 1000)
res_val = af_forward(X_val,Y_val,res_opt$estimate,m,0)
validation_errors[i] = res_val$obj
}
plot(validation_errors~v_values,type = 'b')
#| echo: false
dat = read.table("Collider_Data_2025.txt", h = TRUE,stringsAsFactors =TRUE) # Text columns automatically become factors (categories)
# Load libraries
library(ggplot2)
library(dplyr)
library(r2symbols)
library(tidyr)
library(formatR)
#| echo: false
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
#| echo: true
#| label: fig-scatter
#| fig-cap: "This is a scatter plot of the Collider data in the X1 and X2 feature space, with points colour-coded by true particle class (α in red, β in blue, ρ in green), illustrating non-linear boundaries that motivate using a neural network for classification."
dat$response <- apply(dat[, c("Y1", "Y2", "Y3")], 1, function (x){
if (x[1] == 1) return("code-Alpha")
if (x[2] == 1) return("code-Beta")
if (x[3] == 1) return("code-Rho")
})
ggplot(dat, aes(x=X1, y=X2, color=response))+
geom_point(size=2)+
coord_fixed()+ # 1:1 aspect ratio
labs(title = "Scatterplot of particles in feature space",
x = "First coordinate (X1)", y = "Second coordinate (X2)",
color = "Particle type")+
theme_minimal()
#| echo: true
softmax <- function(Z)
{
Z_shift <- Z - matrix(apply(Z, 2, max), nrow = 3, ncol = ncol(Z),
byrow = TRUE)
#Subtract the column max for numerical stability (to avoid computational overflow when exponentiating)
expZ    <- exp(Z_shift)
denom   <- matrix(colSums(expZ),
nrow = 3, ncol = ncol(Z), byrow =TRUE)
# column-wise sums...sum across 3 classes
# convert to matrix for conformability
expZ / denom
}
#| echo: true
g <- function(Yhat, Y, eps = 1e-15) {
# Yhat, Y : N × q matrices   (rows = observations, columns = classes)
N <- nrow(Y)
-sum( Y * log( pmax(Yhat, eps) ) ) / N
# pmax() replaces any element of Yhat that is smaller than eps with eps   #ensures no value passed to log() is <= zero
}
#| echo: true
# X: input matrix (N x p)
# Y: output matrix (N x q)
# theta: parameter vector with all weights and biases
# m: number of nodes on hidden layer
# v: regularisation parameter
af_forward <- function(X, Y, theta, m, v)
{
N <- nrow(X)
p <- ncol(X)
q <- ncol(Y)
# Populate weight-matrix and bias vectors by unpacking theta:
index <- 1:(2*(p^2)) #W1 : p(p+p)
W1 <- matrix(theta[index], nrow=p)
index <- max(index)+1:(2*p) #b1 : (p+p)
b1 <- theta[index]
index <- max(index)+1:((2*p)*m) #W2 : (p+p)*m
W2 <- matrix(theta[index], nrow=2*p)
index <- max(index)+1:m #b2 : m
b2 <- theta[index]
index <- max(index)+1:(m*m) #W3 : (m*m)
W3 <- matrix(theta[index], nrow=m)
index <- max(index)+1:m #b3 : m
b3 <- theta[index]
index <- max(index)+1:(m*q) #W4 : (m*q)
W4 <- matrix(theta[index], nrow=m)
index <- max(index)+1:q #b4 : q
b4 <- theta[index]
#forward propagation
H1 <- tanh( X  %*% W1 + matrix(b1, N, 2*p, TRUE) ) # aug-layer output
H2 <- tanh( H1 %*% W2 + matrix(b2, N, m, TRUE) ) # 1st hidden layer output
H3 <- tanh(H2 %*% W3 + matrix(b3, N, m, TRUE)) # 2nd hidden layer output
Z <- H3 %*% W4 + matrix(b4, N, q, TRUE) # final layer to get logits
# apply softmax across logits
P_3byN <- softmax(t(Z))
# temporarily transpose because softmax expects input where columns are different samples
probs   <- t(P_3byN)
#losses & objective
loss <- g(probs, Y) # cross-entropy
obj  <- loss + (v / 2) * sum(theta^2) # L2 regularisation
list(probs = probs, loss = loss, obj = obj)
}
set.seed(2025)
m <- 4
# Step 1: Split the data into training and validation sets (80%/20%)
n <- nrow(dat)
train_size <- floor(0.8 * n)
train_indices <- sample(1:n, train_size)
train_data <- dat[train_indices, ]
valid_data <- dat[-train_indices, ]
# Step 2: Prepare the training and validation datasets
X_train <- as.matrix(train_data[, 1:3])  # Input features
Y_train <- as.matrix(train_data[, 4:6])  # Response variables (one-hot encoded)
X_valid <- as.matrix(valid_data[, 1:3])  # Input features
Y_valid <- as.matrix(valid_data[, 4:6])  # Response variables (one-hot encoded)
# Step 3: Define the objective function with regularization
v=0.01
obj_pen <- function(pars) {
result <- af_forward(X_train, Y_train, theta, m, v)
return(result$obj)
}
theta_rand = runif(npars,-1,1)
npars  <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
# Initial random theta
p <- ncol(X_train)
q <- ncol(Y_train)
m <- 4
npars  <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
theta_rand = runif(npars,-1,1)
obj_pen(theta_rand)
obj_pen <- function(theta) {
result <- af_forward(X_train, Y_train, theta, m, v)
return(result$obj)
}
# Initial random theta
p <- ncol(X_train)
q <- ncol(Y_train)
m <- 4
npars  <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
theta_rand = runif(npars,-1,1)
obj_pen(theta_rand)
res_opt = nlm(obj_pen,theta_rand,iterlim = 1000)
res_opt
# Step 4: Grid search over regularization parameter nu
n_nu <- 15
validation_errors = rep(NA,n_nu)
v_values <- exp(seq(-6, 2, length.out = n_nu))
for (i in 1:n_nu) {
v <- v_values[i]
res_opt = nlm(obj_pen,theta_rand,iterlim = 1000)
res_val = af_forward(X_val,Y_val,res_opt$estimate,m,0)
validation_errors[i] = res_val$obj
}
for (i in 1:n_nu) {
v <- v_values[i]
res_opt = nlm(obj_pen,theta_rand,iterlim = 1000)
res_val = af_forward(X_valid,Y_valid,res_opt$estimate,m,0)
validation_errors[i] = res_val$obj
}
plot(validation_errors~v_values,type = 'b')
plot(validation_errors~v_values,type = 'b', log= "x")
# Step 4: Grid search over regularization parameter nu
n_nu <- 20
validation_errors = rep(NA,n_nu)
v_values <- exp(seq(-8, 2, length.out = n_nu))
for (i in 1:n_nu) {
v <- v_values[i]
res_opt = nlm(obj_pen,theta_rand,iterlim = 1000)
res_val = af_forward(X_valid,Y_valid,res_opt$estimate,m,0)
validation_errors[i] = res_val$obj
}
plot(validation_errors~v_values,type = 'b', log= "x")
# Step 4: Grid search over regularization parameter nu
n_nu <- 25
validation_errors = rep(NA,n_nu)
v_values <- exp(seq(-10, 1, length.out = n_nu))
for (i in 1:n_nu) {
v <- v_values[i]
res_opt = nlm(obj_pen,theta_rand,iterlim = 1000)
res_val = af_forward(X_valid,Y_valid,res_opt$estimate,m,0)
validation_errors[i] = res_val$obj
}
plot(validation_errors~v_values,type = 'b', log= "x")
plot(validation_errors~v_values,type = 'b', log= "x", pch=16)
plot(validation_errors~v_values,type = 'b', log= "x", pch=16)
best_i  <- which.min(validation_errors)
best_v <- v_values[best_i]
plot(validation_errors~v_values,type = 'b', log= "x", pch=16, xlab = expression(nu),
ylab = "Validation Cross‐Entropy Loss",
main = "Validation Loss vs Regularization (m = 4)")
abline(v = best_nu, col = "red", lty = 2)
plot(validation_errors~v_values,type = 'b', log= "x", pch=16, xlab = expression(nu),
ylab = "Validation Cross‐Entropy Loss",
main = "Validation Loss vs Regularization (m = 4)")
abline(v = best_nu, col = "red", lty = 2)
plot(validation_errors~v_values,type = 'b', log= "x", pch=16, xlab = expression(nu),
ylab = "Validation Cross‐Entropy Loss",
main = "Validation Loss vs Regularization (m = 4)")
abline(v = best_v, col = "red", lty = 2)
points(best_v, validation_errors[best_i], col = "red", pch = 16)
legend("topright",
legend = paste0("Chosen ν = ", signif(best_v,3)),
col    = "red", lty = 2, pch = 16, bty = "n")
plot(validation_errors~v_values,type = 'b', log= "x", pch=16, xlab = expression(nu),
ylab = "Validation Cross‐Entropy Loss",
main = "Validation Loss vs Regularization (m = 4)", asp=1)
abline(v = best_v, col = "red", lty = 2)
points(best_v, validation_errors[best_i], col = "red", pch = 16)
legend("topright",
legend = paste0("Chosen ν = ", signif(best_v,3)),
col    = "red", lty = 2, pch = 16, bty = "n")
plot(v_values, validation_errors, type = 'b', log = "x", pch = 16,
xlab = expression(nu),
ylab = "Validation Cross‐Entropy Loss",
main = "Validation Loss vs Regularization (m = 4)",
asp = 1)
abline(v = best_v, col = "red", lty = 2)
points(best_v, validation_errors[best_i], col = "red", pch = 16)
legend("topright",
legend = paste0("Chosen ν = ", signif(best_v,3)),
col    = "red", lty = 2, pch = 16, bty = "n")
par(pty="s")
plot(v_values, validation_errors, type = 'b', log = "x", pch = 16,
xlab = expression(nu),
ylab = "Validation Cross‐Entropy Loss",
main = "Validation Loss vs Regularization (m = 4)",
asp = 1)
abline(v = best_v, col = "red", lty = 2)
points(best_v, validation_errors[best_i], col = "red", pch = 16)
legend("topright",
legend = paste0("Chosen ν = ", signif(best_v,3)),
col    = "red", lty = 2, pch = 16, bty = "n")
setwd("~/Documents")
setwd("~/Documents/GitHub/analytics-2")
>>>>>>> e5f2842f7ebe83064a019aaef75969ca4fb49224

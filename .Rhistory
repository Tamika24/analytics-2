index <- max(index)+1:m #b2 : m
b2 <- theta[index]
index <- max(index)+1:(m*m) #W3 : (m*m)
W3 <- matrix(theta[index], nrow=m)
index <- max(index)+1:m #b3 : m
b3 <- theta[index]
index <- max(index)+1:(m*q) #W4 : (m*q)
W4 <- matrix(theta[index], nrow=m)
index <- max(index)+1:q #b4 : q
b4 <- theta[index]
#softmax function but changed to row wise because of conformability?
# softmax <- function(Z) {
#Zs   <- Z - matrix(apply(Z, 1, max), nrow = nrow(Z), ncol = ncol(Z), byrow = FALSE)
#expZ <- exp(Zs)
#expZ / matrix(rowSums(expZ), nrow = nrow(Z), ncol = ncol(Z), byrow = FALSE)
#}
# softmax <- function(Z)
#{
#Z_shift <- Z - matrix(apply(Z, 2, max), nrow = 3, ncol = ncol(Z),byrow = TRUE)
#expZ    <- exp(Z_shift)                          # exponentiate
#denom   <- matrix(colSums(expZ),                 # column-wise sums
#                 nrow = 3, ncol = ncol(Z),
#                byrow = TRUE)
#expZ / denom
#}
#forward propagation
H1 <- tanh( X  %*% W1 + matrix(b1, N, 2*p, TRUE) )       # aug-layer
H2 <- tanh( H1 %*% W2 + matrix(b2, N, m, TRUE) )      # 2nd hidden
H3 <- tanh(H2 %*% W3 + matrix(b3, N, m, TRUE))
Z <- H3 %*% W4 + matrix(b4, N, q, TRUE) # logits
#used the colsums in softmax but then transposed these probabilities
P_3byN <- softmax(t(Z))   # t(Z) is q×N but q=3 here
probs   <- t(P_3byN)
#probs <- softmax(Z) didnt work because of un-conformable matrices when i used the colsums in softmax and didn't transpose anything
#losses & objective
loss <- g(probs, Y)                 # cross-entropy
obj  <- loss + (nu / 2) * sum(theta^2)
list(probs = probs, loss = loss, obj = obj)
}
(p + 1)m + (m + 1)m + (m + 1)q.
set.seed(2025)
n <- nrow(dat)
train_idx <- sample(1:n, size = 0.8 * n)
dat_train <- dat[train_idx,]
dat_valid <- dat[-train_idx,]
# Prepare input/output
X_train <- as.matrix(dat_train[,1:3])
Y_train <- as.matrix(dat_train[,4:6])
X_valid <- as.matrix(dat_valid[,1:3])
Y_valid <- as.matrix(dat_valid[,4:6])
# Grid of nu values
nu_vals <- exp(seq(-6, 2, length = 50))
val_losses <- numeric(length(nu_vals))
for (i in seq_along(nu_vals)) {
# Random initialisation
params <- list(
W1 = matrix(rnorm((3+1)*4, sd=0.1), nrow=4),
W2 = matrix(rnorm((4+1)*4, sd=0.1), nrow=4),
W3 = matrix(rnorm((4+1)*3, sd=0.1), nrow=3)
)
out <- forward_AFnetwork(X_valid, Y_valid, params, nu_vals[i])
val_losses[i] <- out$loss
}
# Set seed and split data
set.seed(2025)
N <- nrow(X)
af_forward <- function(X, Y, theta, m, nu)
{
N <- nrow(X)
p <- ncol(X)
q <- ncol(Y)
index <- 1:(2*(p^2)) #W1 : p(p+p)
W1 <- matrix(theta[index], nrow=p)
index <- max(index)+1:(2*p) #b1 : (p+p)
b1 <- theta[index]
index <- max(index)+1:((2*p)*m) #W2 : (p+p)*m
W2 <- matrix(theta[index], nrow=2*p)
index <- max(index)+1:m #b2 : m
b2 <- theta[index]
index <- max(index)+1:(m*m) #W3 : (m*m)
W3 <- matrix(theta[index], nrow=m)
index <- max(index)+1:m #b3 : m
b3 <- theta[index]
index <- max(index)+1:(m*q) #W4 : (m*q)
W4 <- matrix(theta[index], nrow=m)
index <- max(index)+1:q #b4 : q
b4 <- theta[index]
#softmax function but changed to row wise because of conformability?
# softmax <- function(Z) {
#Zs   <- Z - matrix(apply(Z, 1, max), nrow = nrow(Z), ncol = ncol(Z), byrow = FALSE)
#expZ <- exp(Zs)
#expZ / matrix(rowSums(expZ), nrow = nrow(Z), ncol = ncol(Z), byrow = FALSE)
#}
# softmax <- function(Z)
#{
#Z_shift <- Z - matrix(apply(Z, 2, max), nrow = 3, ncol = ncol(Z),byrow = TRUE)
#expZ    <- exp(Z_shift)                          # exponentiate
#denom   <- matrix(colSums(expZ),                 # column-wise sums
#                 nrow = 3, ncol = ncol(Z),
#                byrow = TRUE)
#expZ / denom
#}
#forward propagation
H1 <- tanh( X  %*% W1 + matrix(b1, N, 2*p, TRUE) )       # aug-layer
H2 <- tanh( H1 %*% W2 + matrix(b2, N, m, TRUE) )      # 2nd hidden
H3 <- tanh(H2 %*% W3 + matrix(b3, N, m, TRUE))
Z <- H3 %*% W4 + matrix(b4, N, q, TRUE) # logits
#used the colsums in softmax but then transposed these probabilities
P_3byN <- softmax(t(Z))   # t(Z) is q×N but q=3 here
probs   <- t(P_3byN)
#probs <- softmax(Z) didnt work because of un-conformable matrices when i used the colsums in softmax and didn't transpose anything
#losses & objective
loss <- g(probs, Y)                 # cross-entropy
obj  <- loss + (nu / 2) * sum(theta^2)
list(probs = probs, loss = loss, obj = obj)
}
# Set seed and split data
set.seed(2025)
N <- nrow(X)
# Set seed and split data
set.seed(2025)
N <- nrow(dat)
set <- sample(1:N, size = 0.8 * N, replace = FALSE)
dat_train <- dat[set, , drop = FALSE]
dat_valid <- dat[-set, , drop = FALSE]
# Prepare input/output matrices
X_train <- as.matrix(dat_train[, c("X1", "X2", "X3")])
Y_train <- as.matrix(dat_train[, c("Y1", "Y2", "Y3")])
X_val   <- as.matrix(dat_valid[, c("X1", "X2", "X3")])
Y_val   <- as.matrix(dat_valid[, c("Y1", "Y2", "Y3")])
# Network dimensions and parameter count
m    <- 4
p    <- ncol(X_train)
q    <- ncol(Y_train)
npar <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
# Penalized training objective (nu will be set inside loop)
obj_pen <- function(theta) {
af_forward(X_train, Y_train, theta, m, nu)$obj
}
# Random initialization of parameters
theta_rand <- runif(npar, -1, 1)
# Sequence of regularization levels
n_nu   <- 20
nu_seq <- exp(seq(-10, -2, length = n_nu))
# Initialize validation loss vector
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu <- nu_seq[i]  # Set global nu
# Optimize penalized objective on training set
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# Evaluate unpenalized validation loss
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
# Set seed and split data
set.seed(2025)
N <- nrow(dat)
set <- sample(1:N, size = 0.8 * N, replace = FALSE)
dat_train <- dat[set, , drop = FALSE]
dat_valid <- dat[-set, , drop = FALSE]
# Prepare input/output matrices
X_train <- as.matrix(dat_train[, c("X1", "X2", "X3")])
Y_train <- as.matrix(dat_train[, c("Y1", "Y2", "Y3")])
X_val   <- as.matrix(dat_valid[, c("X1", "X2", "X3")])
Y_val   <- as.matrix(dat_valid[, c("Y1", "Y2", "Y3")])
# Network dimensions and parameter count
m    <- 4
p    <- ncol(X_train)
q    <- ncol(Y_train)
npar <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
# Penalized training objective (nu will be set inside loop)
obj_pen <- function(theta) {
af_forward(X_train, Y_train, theta, m, nu)$obj
}
# Random initialization of parameters
theta_rand <- runif(npar, -1, 1)
# Sequence of regularization levels
n_nu   <- 20
nu_seq <- exp(seq(-10, -2, length = n_nu))
# Initialize validation loss vector
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu <- nu_seq[i]  # Set global nu
# Optimize penalized objective on training set
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# Evaluate unpenalized validation loss
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
x
# Set seed and split data
set.seed(2025)
N <- nrow(dat)
set <- sample(1:N, size = 0.8 * N, replace = FALSE)
dat_train <- dat[set, , drop = FALSE]
dat_valid <- dat[-set, , drop = FALSE]
# Prepare input/output matrices
X_train <- as.matrix(dat_train[, c("X1", "X2", "X3")])
Y_train <- as.matrix(dat_train[, c("Y1", "Y2", "Y3")])
X_val   <- as.matrix(dat_valid[, c("X1", "X2", "X3")])
Y_val   <- as.matrix(dat_valid[, c("Y1", "Y2", "Y3")])
# Network dimensions and parameter count
m    <- 4
p    <- ncol(X_train)
q    <- ncol(Y_train)
npar <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
# Penalized training objective (nu will be set inside loop)
obj_pen <- function(theta) {
af_forward(X_train, Y_train, theta, m, nu)$obj
}
# Random initialization of parameters
theta_rand <- runif(npar, -1, 1)
# Sequence of regularization levels
n_nu   <- 20
nu_seq <- exp(seq(-10, -2, length = n_nu))
# Initialize validation loss vector
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu <- nu_seq[i]  # Set global nu
# Optimize penalized objective on training set
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# Evaluate unpenalized validation loss
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
# Set seed and split data
set.seed(2025)
N <- nrow(dat)
set <- sample(1:N, size = 0.8 * N, replace = FALSE)
dat_train <- dat[set, , drop = FALSE]
dat_valid <- dat[-set, , drop = FALSE]
# Prepare input/output matrices
X_train <- as.matrix(dat_train[, c("X1", "X2", "X3")])
Y_train <- as.matrix(dat_train[, c("Y1", "Y2", "Y3")])
X_val   <- as.matrix(dat_valid[, c("X1", "X2", "X3")])
Y_val   <- as.matrix(dat_valid[, c("Y1", "Y2", "Y3")])
# Network dimensions and parameter count
m    <- 4
p    <- ncol(X_train)
q    <- ncol(Y_train)
npar <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
# Penalized training objective (nu will be set inside loop)
obj_pen <- function(theta) {
af_forward(X_train, Y_train, theta, m, nu)$obj
}
# Random initialization of parameters
theta_rand <- runif(npar, -1, 1)
# Sequence of regularization levels
n_nu   <- 20
nu_seq <- exp(seq(-10, -2, length = n_nu))
# Initialize validation loss vector
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu <- nu_seq[i]  # Set global nu
# Optimize penalized objective on training set
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# Evaluate unpenalized validation loss
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
#prepare data
X <- as.matrix(dat[, c("X1", "X2", "X3")])
Y <- as.matrix(dat[, c("Y1", "Y2", "Y3")])
# Set seed and split data
set.seed(2025)
N <- nrow(X)
set <- sample(1:N, size = 0.8 * N)
X_train <- X[set, , drop = FALSE]
Y_train <- Y[set, , drop = FALSE]
X_val   <- X[-set, , drop = FALSE]
Y_val   <- Y[-set, , drop = FALSE]
# Network dimensions and parameter count
m    <- 4
p    <- ncol(X_train)
q    <- ncol(Y_train)
npar <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
# Penalized training objective (nu will be set inside loop)
obj_pen <- function(theta) {
af_forward(X_train, Y_train, theta, m, nu)$obj
}
# Random initialization of parameters
theta_rand <- runif(npar, -1, 1)
# Sequence of regularization levels
n_nu   <- 20
nu_seq <- exp(seq(-10, -2, length = n_nu))
# Initialize validation loss vector
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu <- nu_seq[i]  # Set global nu
# Optimize penalized objective on training set
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# Evaluate unpenalized validation loss
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
#logged x to should more of the u shape but seems to be plotting something not right ahah so maybe not this one
#plot(nu_seq, Val_error, type="b", pch=16, log="x",
#     xlab = expression(nu), ylab = "Validation Loss",
#    main = "Validation Loss vs Regularization (m = 4)")
#abline(v = nu_seq[best_i], col = "red", lty = 2)
#legend("topright",
#       legend = paste("min at ν =", signif(nu_seq[best_i],3)),
#      col = "red", lty = 2, bty="n")
# Set seed and split data
set.seed(2025)
N <- nrow(X)
set <- sample(1:N, size = 0.8 * N)
X_train <- X[set, , drop = FALSE]
Y_train <- Y[set, , drop = FALSE]
X_val   <- X[-set, , drop = FALSE]
Y_val   <- Y[-set, , drop = FALSE]
# Network dimensions and parameter count
m    <- 4
p    <- ncol(X_train)
q    <- ncol(Y_train)
npar <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
# Penalized training objective (nu will be set inside loop)
obj_pen <- function(theta) {
af_forward(X_train, Y_train, theta, m, nu)$obj
}
# Random initialization of parameters
theta_rand <- runif(npar, -1, 1)
# Sequence of regularization levels
n_nu   <- 20
nu_seq <- exp(seq(-10, -2, length = n_nu))
# Initialize validation loss vector
Val_error <- numeric(n_nu)
# Plot validation curve
plot(nu_seq, Val_error, type = "b", pch = 16,
xlab = expression(nu), ylab = "Validation Loss",
main = "Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
# Plot validation curve
plot(nu_seq, Val_error, type = "b", pch = 16,
xlab = expression(nu), ylab = "Validation Loss",
main = "Validation Loss vs Regularization (m = 4)")
points(best_nu, Val_error[best_i], col = "red", pch = 16)
best_i  <- which.min(Val_error)
best_nu <- nu_seq[best_i]
points(best_nu, Val_error[best_i], col = "red", pch = 16)
legend("topright",
legend = paste("Chosen ν =", signif(best_nu, 3)),
col = "red", pch = 16, bty = "n")
plot(nu_seq, Val_error, type = "b", pch = 16,
xlab = expression(nu), ylab = "Validation Loss",
main = "Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
best_nu <- nu_seq[best_i]
points(best_nu, Val_error[best_i], col = "red", pch = 16)
legend("topright",
legend = paste("Chosen ν =", signif(best_nu, 3)),
col = "red", pch = 16, bty = "n")
#Split into 80% training / 20% validation
set.seed(2025)
set    <- sample(1:N, 0.8 * N, FALSE)
X_train <- X[set, , drop = FALSE]
X_val   <- X[-set, , drop = FALSE]
#Network dims & parameter count
m     <- 4
q     <- ncol(Y_train)
#Penalised training objective (nu is global)
nu      <- 0.01
obj_pen <- function(pars) {
af_forward(X_train, Y_train, pars, m, nu)$obj
}
#Initialize random theta
theta_rand <- runif(npar, -1, 1)
#Sequence of regularisation levels
n_nu   <- 20
#expanded the grid to show more of the u shape
nu_seq <- exp(seq(-10, -2, length = n_nu))
#Optimize and record validation loss
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu      <- nu_seq[i]
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# unpenalized loss on validation
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
for (i in seq_len(n_nu)) {
nu      <- nu_seq[i]
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
# unpenalized loss on validation
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
# 7. Plot validation curve
plot(nu_seq, Val_error, type="b", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
best_nu <- nu_seq[best_i]
points(best_nu, Val_error[best_i], col="red", pch=16)
legend("topright",
legend=paste("Chosen ν =", signif(best_nu, 3)),
col="red", pch=16, bty="n")
# 7. Plot validation curve
plot(nu_seq, Val_error, type="b", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
# 7. Plot validation curve
plot(nu_seq, Val_error, type="b", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
points(best_nu, Val_error[best_i], col="red", pch=16)
legend("topright",
legend=paste("Chosen ν =", signif(best_nu, 3)),
col="red", pch=16, bty="n")
plot(nu_seq, Val_error, type="b", pch=16,
xlab=expression(nu), ylab="Validation Loss",
main="Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
best_nu <- nu_seq[best_i]
points(best_nu, Val_error[best_i], col="red", pch=16)
legend("topright",
legend=paste("Chosen ν =", signif(best_nu, 3)),
col="red", pch=16, bty="n")
# Set seed and split data
set.seed(2025)
set <- sample(1:N, size = 0.8 * N, replace = FALSE)
dat_valid <- dat[-set, , drop = FALSE]
Y_train <- as.matrix(dat_train[, c("Y1", "Y2", "Y3")])
Y_val   <- as.matrix(dat_valid[, c("Y1", "Y2", "Y3")])
p    <- ncol(X_train)
npar <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
# Sequence of regularization levels
n_nu   <- 20
nu_seq <- exp(seq(-10, -2, length = n_nu))
# Initialize validation loss vector
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu <- nu_seq[i]  # Set global nu
# NEW: Random reinitialization of parameters at each nu
theta_rand <- runif(npar, -1, 1)
# Optimize penalized objective on training set
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 2000)  # Increase iterlim
# Evaluate unpenalized validation loss
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
for (i in seq_len(n_nu)) {
nu <- nu_seq[i]  # Set global nu
# NEW: Random reinitialization of parameters at each nu
theta_rand <- runif(npar, -1, 1)
# Optimize penalized objective on training set
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 2000)  # Increase iterlim
# Evaluate unpenalized validation loss
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
# Plot validation curve
plot(log(nu_seq), Val_error, type = "b", pch = 16,
xlab = expression(log(nu)), ylab = "Validation Loss",
main = "Validation Loss vs Regularization (m = 4)")
for (i in seq_len(n_nu)) {
nu <- nu_seq[i]  # Set global nu
# NEW: Random reinitialization of parameters at each nu
theta_rand <- runif(npar, -1, 1)
# Optimize penalized objective on training set
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 2000)  # Increase iterlim
# Evaluate unpenalized validation loss
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
for (i in seq_len(n_nu)) {
nu <- nu_seq[i]  # Set global nu
# NEW: Random reinitialization of parameters at each nu
theta_rand <- runif(npar, -1, 1)
# Optimize penalized objective on training set
res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 2000)  # Increase iterlim
# Evaluate unpenalized validation loss
Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}
# Set seed and split data
set.seed(2025)
N <- nrow(dat)
set <- sample(1:N, size = 0.8 * N, replace = FALSE)
dat_train <- dat[set, , drop = FALSE]
dat_valid <- dat[-set, , drop = FALSE]
# Prepare input/output matrices
X_train <- as.matrix(dat_train[, c("X1", "X2", "X3")])
Y_train <- as.matrix(dat_train[, c("Y1", "Y2", "Y3")])
X_val   <- as.matrix(dat_valid[, c("X1", "X2", "X3")])
Y_val   <- as.matrix(dat_valid[, c("Y1", "Y2", "Y3")])
# Network dimensions and parameter count
m    <- 4
p    <- ncol(X_train)
q    <- ncol(Y_train)
npar <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q
# Penalized training objective (nu will be set inside loop)
obj_pen <- function(theta) {
af_forward(X_train, Y_train, theta, m, nu)$obj
}
# Sequence of regularization levels
n_nu   <- 20
nu_seq <- exp(seq(-10, -2, length = n_nu))
# Initialize validation loss vector
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
nu <- nu_seq[i]  # Set global nu
# NEW: Random but small initialization
theta_rand <- runif(npar, -0.5, 0.5)
# Optimize penalized objective
res_opt <- optim(theta_rand, obj_pen, method = "BFGS",
control = list(maxit = 500))   # Limit to 500 iterations max
# Evaluate unpenalized validation loss
Val_error[i] <- af_forward(X_val, Y_val, res_opt$par, m, nu = 0)$loss
cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}

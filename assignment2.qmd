---
title: "Assignment 2"
format: pdf
editor: visual
---

```{r, warning=FALSE, message=FALSE}
#| echo: false
dat = read.table("Collider_Data_2025.txt", h = TRUE,stringsAsFactors =TRUE)
#doesn't want the libraries in code
library(ggplot2)
library(dplyr)
library(r2symbols)
```

# a)

```{r, warning=FALSE, message=FALSE}
#| echo: true
dat$response <- apply(dat[, c("Y1", "Y2", "Y3")], 1, function (x){
  if (x[1] == 1) return("code-Alpha")
  if (x[2] == 1) return("code-Beta")
  if (x[3] == 1) return("code-Rho")
})
ggplot(dat, aes(x=X1, y=X2, color=response ))+geom_point(size=2)+ggtitle("Scatter plot of Collider Data Feature Space")+ theme_minimal()



```

The scatter plot shows how the particles are distributed in the X1-X2 plane. We can see that the process creates a complex and non-linear separation in the points plotted. Therefore, the use of a neural network is justified as they are known for their ability to learn non-linear decision boundaries which is presented in the scatter plot.

# b)

```{r, warning=FALSE, message=FALSE}
#| echo: true
softmax <- function(Z) 
  {
 
  Z_shift <- Z - matrix(apply(Z, 2, max), nrow = 3, ncol = ncol(Z),byrow = TRUE)
  
  expZ    <- exp(Z_shift)                          # exponentiate
  denom   <- matrix(colSums(expZ),                 # column-wise sums
                    nrow = 3, ncol = ncol(Z), 
                    byrow = TRUE)
  
  expZ / denom                                     
  }



```

# c)

MAYBE DO IN OVERLEAF, so its a typed response rather than this code chunk

```{r, eval=FALSE}
#| echo: true
calc_Ci <- function(y_true, y_pred) {
  # y_true is assumed to be a scalar (either 0 or 1)
  if (y_true == 1) {
    return(-log(y_pred))
  } else {
    return(0)
  }
}
```

Evaluating only the component corresponding to the actual class simplifies calculations and enhances numerical stability by avoiding evaluating terms that are known to be 0.

# d)

```{r, warning=FALSE, message=FALSE}
#| echo: true
g <- function(Yhat, Y, eps = 1e-15) {
  # Yhat, Y : N × q matrices   (rows = obs, cols = classes)
  N <- nrow(Y)
  -sum( Y * log( pmax(Yhat, eps) ) ) / N
}

```

# e)

Number of parameters = 2p^2^+2p+2pm+2m+m^2^+mq+q

# f) had to change the softmax to be row wise in this function because of conformability, so not sure if that was right OR in "probs \<- softmax(Z)" use the transpose instead? so softmax(t(Z)). 

so see which one is right or better:

1.  using the row wise softmax in the af_forward function which means we didn't use the function asked for in question b, then use the normal probs(softmax(Z)). commented this out for now in this code chunk, so can delete which ever one we don't want
2.  use the colsums as done in question b, and then used the transposes for the probabilities as it is now in the code currently

```{r, warning=FALSE, message=FALSE}
#| echo: true

af_forward <- function(X, Y, theta, m, nu)
{
  N <- nrow(X)
  p <- ncol(X)
  q <- ncol(Y)
  
  index <- 1:(2*(p^2)) #W1 : p(p+p)
  W1 <- matrix(theta[index], nrow=p)
  
  index <- max(index)+1:(2*p) #b1 : (p+p)
  b1 <- theta[index]
  
  index <- max(index)+1:((2*p)*m) #W2 : (p+p)*m
  W2 <- matrix(theta[index], nrow=2*p)
  
  index <- max(index)+1:m #b2 : m
  b2 <- theta[index]
  
  index <- max(index)+1:(m*m) #W3 : (m*m)
  W3 <- matrix(theta[index], nrow=m)
  
  index <- max(index)+1:m #b3 : m
  b3 <- theta[index]
  
  index <- max(index)+1:(m*q) #W4 : (m*q)
  W4 <- matrix(theta[index], nrow=m)
  
  index <- max(index)+1:q #b4 : q
  b4 <- theta[index]
  
  #softmax function but changed to row wise because of conformability?
 # softmax <- function(Z) {
  #Zs   <- Z - matrix(apply(Z, 1, max), nrow = nrow(Z), ncol = ncol(Z), byrow = FALSE)
  #expZ <- exp(Zs)
  
  #expZ / matrix(rowSums(expZ), nrow = nrow(Z), ncol = ncol(Z), byrow = FALSE)
#}
  softmax <- function(Z) 
  {
 
  Z_shift <- Z - matrix(apply(Z, 2, max), nrow = 3, ncol = ncol(Z),byrow = TRUE)
  
  expZ    <- exp(Z_shift)                          # exponentiate
  denom   <- matrix(colSums(expZ),                 # column-wise sums
                    nrow = 3, ncol = ncol(Z), 
                    byrow = TRUE)
  
  expZ / denom                                     
  }
  #forward propagation 
  H1 <- tanh( X  %*% W1 + matrix(b1, N, 2*p, TRUE) )       # aug-layer                           
  H2 <- tanh( H1 %*% W2 + matrix(b2, N, m, TRUE) )      # 2nd hidden
  H3 <- tanh(H2 %*% W3 + matrix(b3, N, m, TRUE))
  Z <- H3 %*% W4 + matrix(b4, N, q, TRUE) # logits
  
  #used the colsums in softmax but then transposed these probabilities
  P_3byN <- softmax(t(Z))   # t(Z) is q×N but q=3 here
  probs   <- t(P_3byN)  
  #probs <- softmax(Z) didnt work because of un-conformable matrices when i used the colsums in softmax and didn't transpose anything                                         

  #losses & objective  
  loss <- g(probs, Y)                 # cross-entropy
  obj  <- loss + (nu / 2) * sum(theta^2)
  list(probs = probs, loss = loss, obj = obj)

}

```

# g) 

```{r, warning=FALSE, message=TRUE}
#| echo: true

#prepare data
X <- as.matrix(dat[, c("X1", "X2", "X3")])
Y <- as.matrix(dat[, c("Y1", "Y2", "Y3")])

#Split into 80% training / 20% validation
set.seed(2025)                      
N     <- dim(X)[1]
set    <- sample(1:N, 0.8 * N, FALSE)
X_train <- X[set, , drop = FALSE]
Y_train <- Y[set, , drop = FALSE]
X_val   <- X[-set, , drop = FALSE]
Y_val   <- Y[-set, , drop = FALSE]

#Network dims & parameter count
m     <- 4
p     <- ncol(X_train)
q     <- ncol(Y_train)
npar  <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q

#Penalised training objective (nu is global)
nu      <- 0.01
obj_pen <- function(pars) {
  af_forward(X_train, Y_train, pars, m, nu)$obj
}

#Initialize random theta
theta_rand <- runif(npar, -1, 1)

#Sequence of regularisation levels
n_nu   <- 20
#expanded the grid to show more of the u shape
nu_seq <- exp(seq(-10, -2, length = n_nu)) 

#Optimize and record validation loss
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
  nu      <- nu_seq[i]
  res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
  
  # unpenalized loss on validation
  Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
  
  cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}

# 7. Plot validation curve
plot(nu_seq, Val_error, type="b", pch=16,
     xlab=expression(nu), ylab="Validation Loss",
     main="Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
best_nu <- nu_seq[best_i]
points(best_nu, Val_error[best_i], col="red", pch=16)
legend("topright",
       legend=paste("Chosen ν =", signif(best_nu, 3)),
       col="red", pch=16, bty="n")

#logged x to should more of the u shape but seems to be plotting something not right ahah so maybe not this one
#plot(nu_seq, Val_error, type="b", pch=16, log="x",
#     xlab = expression(nu), ylab = "Validation Loss",
#    main = "Validation Loss vs Regularization (m = 4)")
#abline(v = nu_seq[best_i], col = "red", lty = 2)
#legend("topright",
#       legend = paste("min at ν =", signif(nu_seq[best_i],3)),
#      col = "red", lty = 2, bty="n")

```

# h)

```{r, warning=FALSE, message=TRUE}
#| echo: true


```

# i) (check and put into own words)

1.  Automated feature engineering. By dedicating its first hidden layer to produce exactly one learned transformation per original input, the network discovers useful nonlinear features without any manual preprocessing. This not only speeds up model development (no need to hand‐craft input transformations) but also often improves sample efficiency and convergence, since the model is guided to focus on one meaningful augmentation per variable.

2.  Structured, interpretable representations and better inductive bias. Because each augmented feature is tied back to a single original input, you can inspect and interpret exactly how the network is transforming each variable. That structural constraint also acts as an inductive bias—rather than letting a large, unstructured hidden layer freely combine everything, the AFnetwork builds in a sensible two‐stage feature expansion that often generalizes better (especially when data are limited) and keeps the total parameter count more controlled.

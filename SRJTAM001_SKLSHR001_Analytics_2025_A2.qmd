---
title: "STA4026S Assignment 2 - Collider Data Classification"
author: "Shriyaa Sooklal SKLSHR001 and Tamika Surajpal SRJTAM001"
format: pdf
editor: visual
---

```{r, warning=FALSE, message=FALSE}
#| echo: false
dat = read.table("Collider_Data_2025.txt", h = TRUE,stringsAsFactors =TRUE)
#doesn't want the libraries in code
library(ggplot2)
library(dplyr)
library(r2symbols)
```

# a)

```{r, warning=FALSE, message=FALSE}
#| echo: true
dat$response <- apply(dat[, c("Y1", "Y2", "Y3")], 1, function (x){
  if (x[1] == 1) return("code-Alpha")
  if (x[2] == 1) return("code-Beta")
  if (x[3] == 1) return("code-Rho")
})
ggplot(dat, aes(x=X1, y=X2, color=response ))+geom_point(size=2)+ggtitle("Scatter plot of Collider Data Feature Space")+ theme_minimal()



```

The scatter plot shows how the particles are distributed in the X1-X2 plane. The scatterplot of the collider data feature space shows three distinct clusters corresponding to the particle types: `code-Alpha`, `code-Beta`, and `code-Rho`. While the classes are broadly separated — with `code-Alpha` particles primarily located on the right, `code-Beta` particles on the left, and `code-Rho` particles clustered near the center — the boundaries between these regions are clearly non-linear. This structure suggests that linear classifiers would struggle to accurately separate the classes.\

Thus, using non-linear models such as a neural network is appropriate for this classification task, as they can capture the curved, complex boundaries observed in the feature space.

# b)

```{r, warning=FALSE, message=FALSE}
#| echo: true
softmax <- function(Z) 
  {
 
  Z_shift <- Z - matrix(apply(Z, 2, max), nrow = 3, ncol = ncol(Z),byrow = TRUE)
  
  expZ    <- exp(Z_shift)                          # exponentiate
  denom   <- matrix(colSums(expZ),                 # column-wise sums
                    nrow = 3, ncol = ncol(Z), 
                    byrow = TRUE)
  
  expZ / denom                                     
  }

#softmax <- function(Z) {
   # exp_Z <- exp(Z)
   # exp_Z / matrix(colSums(exp_Z), nrow = nrow(exp_Z), ncol = ncol(exp_Z), byrow = TRUE)}

```

# c)

The cross-entropy loss for a single observation can be written as:

```{r, eval=FALSE}
#| echo: true
calc_Ci <- function(y_true, y_pred) {
  # y_true is assumed to be a scalar (either 0 or 1)
  if (y_true == 1) {
    return(-log(y_pred))
  } else {
    return(0)
  }
}
```

Evaluating only the component corresponding to the actual class simplifies calculations and enhances numerical stability by avoiding evaluating terms that are known to be 0. This avoids taking log(0) of predicted probabilities for classes that were not selected.

# d)

```{r, warning=FALSE, message=FALSE}
#| echo: true
g <- function(Yhat, Y, eps = 1e-15) {
  # Yhat, Y : N × q matrices   (rows = obs, cols = classes)
  N <- nrow(Y)
  -sum( Y * log( pmax(Yhat, eps) ) ) / N
}

```

# e)

Number of parameters = 2p^2^+2p+2pm+2m+m^2^+mq+q

# f)

```{r, warning=FALSE, message=FALSE}
#| echo: true

af_forward <- function(X, Y, theta, m, nu)
{
  N <- nrow(X)
  p <- ncol(X)
  q <- ncol(Y)
  
  index <- 1:(2*(p^2)) #W1 : p(p+p)
  W1 <- matrix(theta[index], nrow=p)
  
  index <- max(index)+1:(2*p) #b1 : (p+p)
  b1 <- theta[index]
  
  index <- max(index)+1:((2*p)*m) #W2 : (p+p)*m
  W2 <- matrix(theta[index], nrow=2*p)
  
  index <- max(index)+1:m #b2 : m
  b2 <- theta[index]
  
  index <- max(index)+1:(m*m) #W3 : (m*m)
  W3 <- matrix(theta[index], nrow=m)
  
  index <- max(index)+1:m #b3 : m
  b3 <- theta[index]
  
  index <- max(index)+1:(m*q) #W4 : (m*q)
  W4 <- matrix(theta[index], nrow=m)
  
  index <- max(index)+1:q #b4 : q
  b4 <- theta[index]
  
  #forward propagation 
  H1 <- tanh( X  %*% W1 + matrix(b1, N, 2*p, TRUE) )       # aug-layer                           
  H2 <- tanh( H1 %*% W2 + matrix(b2, N, m, TRUE) )      # 2nd hidden
  H3 <- tanh(H2 %*% W3 + matrix(b3, N, m, TRUE))
  Z <- H3 %*% W4 + matrix(b4, N, q, TRUE) # logits
  
  #used the colsums in softmax but then transposed these probabilities
  P_3byN <- softmax(t(Z))   # t(Z) is q×N but q=3 here
  probs   <- t(P_3byN)  

  #losses & objective  
  loss <- g(probs, Y)                 # cross-entropy
  obj  <- loss + (nu / 2) * sum(theta^2)
  list(probs = probs, loss = loss, obj = obj)

}

```

# g)

```{r, warning=FALSE, message=TRUE}
#| echo: true

set.seed(2025)

# Step 1: Split the data into training and validation sets (80%/20%)
n <- nrow(dat)
train_size <- floor(0.8 * n)
train_indices <- sample(1:n, train_size)
train_data <- dat[train_indices, ]
valid_data <- dat[-train_indices, ]

# Step 2: Prepare the training and validation datasets
X_train <- as.matrix(train_data[, 1:3])  # Input features
Y_train <- as.matrix(train_data[, 4:6])  # Response variables (one-hot encoded)

X_valid <- as.matrix(valid_data[, 1:3])  # Input features
Y_valid <- as.matrix(valid_data[, 4:6])  # Response variables (one-hot encoded)

# Step 3: Define the objective function with regularization
objective_fn <- function(theta, X, Y, m, nu) {
  result <- af_forward(X, Y, theta, m, nu)
  return(result$obj)
}

# Step 4: Grid search over regularization parameter nu
nu_values <- exp(seq(-6, 2, length.out = 100))
validation_errors <- numeric(length(nu_values))

for (i in 1:length(nu_values)) {
  nu <- nu_values[i]
  
  # Initial random theta
  p <- ncol(X_train)
  q <- ncol(Y_train)
  m <- 4
  theta_init <- rnorm((2 * p^2) + (2 * p) + (2 * p * m) + m + (m * m) +                 m + (m * q) + q)
  
  # Fit the model using optim() to minimize the objective function
  fit <- optim(theta_init, objective_fn, X = X_train, Y = Y_train, m = 4, nu = nu)
  
  # Get the predicted probabilities for validation set
  Yhat_valid <- af_forward(X_valid, Y_valid, fit$par, m = 4, nu = nu)$probs
  
  # Compute the validation error
  validation_errors[i] <- g(Yhat_valid, Y_valid)
}

# Step 5: Plot validation error vs nu
plot(nu_values, validation_errors, type = "b", col = "blue", pch = 19,
     xlab = "Regularization Parameter (ν)", ylab = "Validation Error",
     main = "Validation Error vs Regularization Parameter (ν)")
grid()

# Step 6: Choose the optimal regularization level (ν)
optimal_nu <- nu_values[which.min(validation_errors)]

```

The optimal regularisation parameter is `r round(optimal_nu, 4)` as it minimises the validation error.

# h)

```{r, warning=FALSE, message=TRUE}
#| echo: true

m <- 4
best_nu <- exp(-2) 
nu <- best_nu

# Re-train model at best nu
obj_pen_best <- function(pars) {
  af_forward(X_train, Y_train, pars, m, nu)$obj
}

theta_rand <- runif(npar, -1, 1)  # Reinitialize random parameters
res_opt_best <- nlm(obj_pen_best, p = theta_rand, iterlim = 1000)
theta_best <- res_opt_best$estimate


# Plot response curves by varying X1 and X2 separately

# Helper function to predict probability curves
predict_curve <- function(var_seq, varname, fixed_X2 = 0, fixed_X3 = 0, pars, m) {
  n <- length(var_seq)
  input <- matrix(0, nrow = n, ncol = 3)
  colnames(input) <- c("X1", "X2", "X3")
  
  input[, "X1"] <- if (varname == "X1") var_seq else fixed_X2
  input[, "X2"] <- if (varname == "X2") var_seq else fixed_X2
  input[, "X3"] <- fixed_X3
  
  q <- 3 
  
  preds <- af_forward(input, Y = matrix(0, nrow=n, ncol=q), pars, m, nu=0)$probs
  
  out <- as.data.frame(preds)
  colnames(out) <- c("alpha", "beta", "rho")
  out[[varname]] <- var_seq
  
  return(out)
}

# Create sequences
X_seq <- seq(-4, 4, length.out = 100)

# Response curves for Detector Type A (X3=1) and Type B (X3=0)

curve_X1_A <- predict_curve(X_seq, "X1", fixed_X2=0, fixed_X3=1, theta_best, m)
curve_X1_B <- predict_curve(X_seq, "X1", fixed_X2=0, fixed_X3=0, theta_best, m)

curve_X2_A <- predict_curve(X_seq, "X2", fixed_X2=0, fixed_X3=1, theta_best, m)
curve_X2_B <- predict_curve(X_seq, "X2", fixed_X2=0, fixed_X3=0, theta_best, m)

# Helper to prepare data
prepare_plot_data <- function(curve_data, varname, type_label) {
  df <- as.data.frame(curve_data)
  colnames(df) <- c(varname, "alpha", "beta", "rho")
  df$Detector <- type_label
  df <- pivot_longer(df, cols = c("alpha", "beta", "rho"),
                     names_to = "Class", values_to = "Probability")
  return(df)
}

plot_data_X1 <- bind_rows(
  prepare_plot_data(curve_X1_A, "X1", "Type A"),
  prepare_plot_data(curve_X1_B, "X1", "Type B")
)

ggplot(plot_data_X1, aes(x = X1, y = Probability, color = Class)) +
  geom_line() +
  facet_wrap(~ Detector) +
  labs(title = "Predicted Class Probabilities vs X1",
       x = "X1", y = "Probability") +
  theme_minimal() +
  theme(aspect.ratio = 1)

plot_data_X2 <- bind_rows(
  prepare_plot_data(curve_X2_A, "X2", "Type A"),
  prepare_plot_data(curve_X2_B, "X2", "Type B")
)

ggplot(plot_data_X2, aes(x = X2, y = Probability, color = Class)) +
  geom_line() +
  facet_wrap(~ Detector) +
  labs(title = "Predicted Class Probabilities vs X2",
       x = "X2", y = "Probability") +
  theme_minimal() +
  theme(aspect.ratio = 1)


```

# i)

Using an Attention-based Feedforward (AF) network over a standard feedforward neural network offers several practical advantages. One of the key benefits is that AF networks can focus on the most relevant parts of the data, allowing them to emphasize the features that matter most for prediction. This helps improve performance in complex scenarios, where relationships between inputs and outputs may not be simple or linear. Standard feedforward networks, on the other hand, treat all inputs equally, which can lead to suboptimal performance in such cases.

Another advantage is that AF networks can capture long-range dependencies between different features, making them more adaptable to data where the importance of features varies over time or across different contexts. For instance, in time-series or sequence-based data, AF networks can weigh the importance of past events or inputs, improving their ability to make accurate predictions.

AF networks also tend to work better with noisy or high-dimensional data. In these situations, a standard feedforward network might get overwhelmed by the irrelevant features, leading to overfitting or underperformance. The attention mechanism helps the network filter out noise and concentrate on the features that genuinely matter, reducing the risk of overfitting and improving generalization.

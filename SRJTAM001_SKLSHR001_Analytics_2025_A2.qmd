---
title: "STA4026S Assignment 2 - Collider Data Classification"
author: "Shriyaa Sooklal SKLSHR001 and Tamika Surajpal SRJTAM001"
format: pdf
editor: visual
---

```{r, warning=FALSE, message=FALSE}
#| echo: false
dat = read.table("Collider_Data_2025.txt", h = TRUE,stringsAsFactors =TRUE)
#doesn't want the libraries in code
library(ggplot2)
library(dplyr)
library(r2symbols)
```

# a)

```{r, warning=FALSE, message=FALSE}
#| echo: true
dat$response <- apply(dat[, c("Y1", "Y2", "Y3")], 1, function (x){
  if (x[1] == 1) return("code-Alpha")
  if (x[2] == 1) return("code-Beta")
  if (x[3] == 1) return("code-Rho")
})
ggplot(dat, aes(x=X1, y=X2, color=response ))+geom_point(size=2)+ggtitle("Scatter plot of Collider Data Feature Space")+ theme_minimal()



```

The scatter plot shows how the particles are distributed in the X1-X2 plane. The scatterplot of the collider data feature space shows three distinct clusters corresponding to the particle types: `code-Alpha`, `code-Beta`, and `code-Rho`. While the classes are broadly separated — with `code-Alpha` particles primarily located on the right, `code-Beta` particles on the left, and `code-Rho` particles clustered near the center — the boundaries between these regions are clearly non-linear. This structure suggests that linear classifiers would struggle to accurately separate the classes.\

Thus, using non-linear models such as a neural network is appropriate for this classification task, as they can capture the curved, complex boundaries observed in the feature space.

# b)

```{r, warning=FALSE, message=FALSE}
#| echo: true
softmax <- function(Z) 
  {
 
  Z_shift <- Z - matrix(apply(Z, 2, max), nrow = 3, ncol = ncol(Z),byrow = TRUE)
  
  expZ    <- exp(Z_shift)                          # exponentiate
  denom   <- matrix(colSums(expZ),                 # column-wise sums
                    nrow = 3, ncol = ncol(Z), 
                    byrow = TRUE)
  
  expZ / denom                                     
  }

#softmax <- function(Z) {
   # exp_Z <- exp(Z)
   # exp_Z / matrix(colSums(exp_Z), nrow = nrow(exp_Z), ncol = ncol(exp_Z), byrow = TRUE)}

```

# c)

MAYBE DO IN OVERLEAF, so its a typed response rather than this code chunk

The cross-entropy loss for a single observation can be written as:

```{r, eval=FALSE}
#| echo: true
calc_Ci <- function(y_true, y_pred) {
  # y_true is assumed to be a scalar (either 0 or 1)
  if (y_true == 1) {
    return(-log(y_pred))
  } else {
    return(0)
  }
}
```

Evaluating only the component corresponding to the actual class simplifies calculations and enhances numerical stability by avoiding evaluating terms that are known to be 0. This avoids taking log(0) of predicted probabilities for classes that were not selected.

# d)

```{r, warning=FALSE, message=FALSE}
#| echo: true
g <- function(Yhat, Y, eps = 1e-15) {
  # Yhat, Y : N × q matrices   (rows = obs, cols = classes)
  N <- nrow(Y)
  -sum( Y * log( pmax(Yhat, eps) ) ) / N
}

```

# e)

Number of parameters = 2p^2^+2p+2pm+2m+m^2^+mq+q

# f) 

had to change the softmax to be row wise in this function because of conformability, so not sure if that was right OR in "probs \<- softmax(Z)" use the transpose instead? so softmax(t(Z)).

so see which one is right or better:

1.  using the row wise softmax in the af_forward function which means we didn't use the function asked for in question b, then use the normal probs(softmax(Z)). commented this out for now in this code chunk, so can delete which ever one we don't want
2.  use the colsums as done in question b, and then used the transposes for the probabilities as it is now in the code currently

*Keep softmax columnwise to match b then inside af_forward apply softmax(t(Z)) then transpose back after*

```{r, warning=FALSE, message=FALSE}
#| echo: true

af_forward <- function(X, Y, theta, m, nu)
{
  N <- nrow(X)
  p <- ncol(X)
  q <- ncol(Y)
  
  index <- 1:(2*(p^2)) #W1 : p(p+p)
  W1 <- matrix(theta[index], nrow=p)
  
  index <- max(index)+1:(2*p) #b1 : (p+p)
  b1 <- theta[index]
  
  index <- max(index)+1:((2*p)*m) #W2 : (p+p)*m
  W2 <- matrix(theta[index], nrow=2*p)
  
  index <- max(index)+1:m #b2 : m
  b2 <- theta[index]
  
  index <- max(index)+1:(m*m) #W3 : (m*m)
  W3 <- matrix(theta[index], nrow=m)
  
  index <- max(index)+1:m #b3 : m
  b3 <- theta[index]
  
  index <- max(index)+1:(m*q) #W4 : (m*q)
  W4 <- matrix(theta[index], nrow=m)
  
  index <- max(index)+1:q #b4 : q
  b4 <- theta[index]
  
  #softmax function but changed to row wise because of conformability?
 # softmax <- function(Z) {
  #Zs   <- Z - matrix(apply(Z, 1, max), nrow = nrow(Z), ncol = ncol(Z), byrow = FALSE)
  #expZ <- exp(Zs)
  
  #expZ / matrix(rowSums(expZ), nrow = nrow(Z), ncol = ncol(Z), byrow = FALSE)
#}
 # softmax <- function(Z) 
  #{
 
  #Z_shift <- Z - matrix(apply(Z, 2, max), nrow = 3, ncol = ncol(Z),byrow = TRUE)
  
  #expZ    <- exp(Z_shift)                          # exponentiate
  #denom   <- matrix(colSums(expZ),                 # column-wise sums
   #                 nrow = 3, ncol = ncol(Z), 
    #                byrow = TRUE)
  
  #expZ / denom                                     
  #}
  #forward propagation 
  H1 <- tanh( X  %*% W1 + matrix(b1, N, 2*p, TRUE) )       # aug-layer                           
  H2 <- tanh( H1 %*% W2 + matrix(b2, N, m, TRUE) )      # 2nd hidden
  H3 <- tanh(H2 %*% W3 + matrix(b3, N, m, TRUE))
  Z <- H3 %*% W4 + matrix(b4, N, q, TRUE) # logits
  
  #used the colsums in softmax but then transposed these probabilities
  P_3byN <- softmax(t(Z))   # t(Z) is q×N but q=3 here
  probs   <- t(P_3byN)  
  #probs <- softmax(Z) didnt work because of un-conformable matrices when i used the colsums in softmax and didn't transpose anything                                         

  #losses & objective  
  loss <- g(probs, Y)                 # cross-entropy
  obj  <- loss + (nu / 2) * sum(theta^2)
  list(probs = probs, loss = loss, obj = obj)

}

```

# g)

```{r, warning=FALSE, message=TRUE}
#| echo: true

#prepare data
X <- as.matrix(dat[, c("X1", "X2", "X3")])
Y <- as.matrix(dat[, c("Y1", "Y2", "Y3")])

#Split into 80% training / 20% validation
set.seed(2025)                      
N     <- dim(X)[1]
set    <- sample(1:N, 0.8 * N, FALSE)
X_train <- X[set, , drop = FALSE]
Y_train <- Y[set, , drop = FALSE]
X_val   <- X[-set, , drop = FALSE]
Y_val   <- Y[-set, , drop = FALSE]

#Network dims & parameter count
m     <- 4
p     <- ncol(X_train)
q     <- ncol(Y_train)
npar  <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q

#Penalised training objective (nu is global)
nu      <- 0.01
obj_pen <- function(pars) {
  af_forward(X_train, Y_train, pars, m, nu)$obj
}

#Initialize random theta
theta_rand <- runif(npar, -1, 1)

#Sequence of regularisation levels
n_nu   <- 20
#expanded the grid to show more of the u shape
nu_seq <- exp(seq(-10, -2, length = n_nu)) 

#Optimize and record validation loss
Val_error <- numeric(n_nu)
for (i in seq_len(n_nu)) {
  nu      <- nu_seq[i]
  res_opt <- nlm(obj_pen, p = theta_rand, iterlim = 1000)
  
  # unpenalized loss on validation
  Val_error[i] <- af_forward(X_val, Y_val, res_opt$estimate, m, nu = 0)$loss
  
  cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}

# 7. Plot validation curve
plot(nu_seq, Val_error, type="b", pch=16,
     xlab=expression(nu), ylab="Validation Loss",
     main="Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
best_nu <- nu_seq[best_i]
points(best_nu, Val_error[best_i], col="red", pch=16)
legend("topright",
       legend=paste("Chosen ν =", signif(best_nu, 3)),
       col="red", pch=16, bty="n")

#logged x to should more of the u shape but seems to be plotting something not right ahah so maybe not this one
#plot(nu_seq, Val_error, type="b", pch=16, log="x",
#     xlab = expression(nu), ylab = "Validation Loss",
#    main = "Validation Loss vs Regularization (m = 4)")
#abline(v = nu_seq[best_i], col = "red", lty = 2)
#legend("topright",
#       legend = paste("min at ν =", signif(nu_seq[best_i],3)),
#      col = "red", lty = 2, bty="n")
# Set seed and split data

# Set seed and split data
set.seed(2025)
N <- nrow(dat)
set <- sample(1:N, size = 0.8 * N, replace = FALSE)
dat_train <- dat[set, , drop = FALSE]
dat_valid <- dat[-set, , drop = FALSE]

# Prepare input/output matrices
X_train <- as.matrix(dat_train[, c("X1", "X2", "X3")])
Y_train <- as.matrix(dat_train[, c("Y1", "Y2", "Y3")])

X_val   <- as.matrix(dat_valid[, c("X1", "X2", "X3")])
Y_val   <- as.matrix(dat_valid[, c("Y1", "Y2", "Y3")])

# Network dimensions and parameter count
m    <- 4
p    <- ncol(X_train)
q    <- ncol(Y_train)
npar <- 2*p^2 + 2*p + 2*p*m + 2*m + m^2 + m*q + q

# Penalized training objective (nu will be set inside loop)
obj_pen <- function(theta) {
  af_forward(X_train, Y_train, theta, m, nu)$obj
}

# Sequence of regularization levels
n_nu   <- 20
nu_seq <- exp(seq(-10, -2, length = n_nu))

# Initialize validation loss vector
Val_error <- numeric(n_nu)

for (i in seq_len(n_nu)) {
  nu <- nu_seq[i]  # Set global nu
  
  # NEW: Random but small initialization
  theta_rand <- runif(npar, -0.5, 0.5)
  
  # Optimize penalized objective
  res_opt <- optim(theta_rand, obj_pen, method = "BFGS",
                   control = list(maxit = 500))   # Limit to 500 iterations max
  
  # Evaluate unpenalized validation loss
  Val_error[i] <- af_forward(X_val, Y_val, res_opt$par, m, nu = 0)$loss
  
  cat("nu =", signif(nu, 3), "→ val loss =", signif(Val_error[i], 4), "\n")
}

# Plot validation curve
plot(log(nu_seq), Val_error, type = "b", pch = 16,
     xlab = expression(log(nu)), ylab = "Validation Loss",
     main = "Validation Loss vs Regularization (m = 4)")
best_i  <- which.min(Val_error)
best_nu <- nu_seq[best_i]
points(log(best_nu), Val_error[best_i], col = "red", pch = 16)
legend("topright",
       legend = paste("Chosen ν =", signif(best_nu, 3)),
       col = "red", pch = 16, bty = "n")

```

# h)

```{r, warning=FALSE, message=TRUE}
#| echo: true

best_nu <- exp(-2) 
# Set best regularization level found earlier
nu <- best_nu

# Re-train model at best nu
obj_pen_best <- function(pars) {
  af_forward(X_train, Y_train, pars, m, nu)$obj
}

theta_rand <- runif(npar, -1, 1)  # Reinitialize random parameters
res_opt_best <- nlm(obj_pen_best, p = theta_rand, iterlim = 1000)
theta_best <- res_opt_best$estimate

# Now: Plot response curves by varying X1 and X2 separately

# Helper function to predict probability curves
predict_curve <- function(var_seq, varname, fixed_X2 = 0, fixed_X3 = 0, pars, m) {
  n <- length(var_seq)
  input <- matrix(0, nrow = n, ncol = 3)
  input[, "X1"] <- if (varname == "X1") var_seq else fixed_X2
  input[, "X2"] <- if (varname == "X2") var_seq else fixed_X2
  input[, "X3"] <- fixed_X3
  
  preds <- af_forward(input, matrix(0, nrow=n, ncol=3), pars, m, nu=0)$Yhat
  cbind(var_seq, preds)
}

# Create sequences
X_seq <- seq(-4, 4, length.out = 100)

# Response curves for Detector Type A (X3=1) and Type B (X3=0)
curve_X1_A <- predict_curve(X_seq, "X1", fixed_X2=0, fixed_X3=1, theta_best, m)
curve_X1_B <- predict_curve(X_seq, "X1", fixed_X2=0, fixed_X3=0, theta_best, m)

curve_X2_A <- predict_curve(X_seq, "X2", fixed_X2=0, fixed_X3=1, theta_best, m)
curve_X2_B <- predict_curve(X_seq, "X2", fixed_X2=0, fixed_X3=0, theta_best, m)

# Now plotting
library(tidyr)
library(dplyr)
library(ggplot2)

# Helper to prepare data
prepare_plot_data <- function(curve_data, varname, type_label) {
  df <- as.data.frame(curve_data)
  colnames(df) <- c(varname, "alpha", "beta", "rho")
  df$Detector <- type_label
  df <- pivot_longer(df, cols = c("alpha", "beta", "rho"),
                     names_to = "Class", values_to = "Probability")
  return(df)
}

plot_data_X1 <- bind_rows(
  prepare_plot_data(curve_X1_A, "X1", "Type A"),
  prepare_plot_data(curve_X1_B, "X1", "Type B")
)

ggplot(plot_data_X1, aes(x = X1, y = Probability, color = Class)) +
  geom_line() +
  facet_wrap(~ Detector) +
  labs(title = "Predicted Class Probabilities vs X1",
       x = "X1", y = "Probability") +
  theme_minimal() +
  theme(aspect.ratio = 1)

plot_data_X2 <- bind_rows(
  prepare_plot_data(curve_X2_A, "X2", "Type A"),
  prepare_plot_data(curve_X2_B, "X2", "Type B")
)

ggplot(plot_data_X2, aes(x = X2, y = Probability, color = Class)) +
  geom_line() +
  facet_wrap(~ Detector) +
  labs(title = "Predicted Class Probabilities vs X2",
       x = "X2", y = "Probability") +
  theme_minimal() +
  theme(aspect.ratio = 1)

```

The response curves show clear differences between Detector Type A and Detector Type B.\

For example, for certain ranges of X1 or X2, the predicted probabilities for the same input differ depending on the detector used.\

This suggests that the model has learned to adjust classification boundaries based on the detector type (X3), confirming that detector characteristics impact the scattering patterns observed.

# i) (check and put into own words)

1.  Automated feature engineering. By dedicating its first hidden layer to produce exactly one learned transformation per original input, the network discovers useful nonlinear features without any manual preprocessing. This not only speeds up model development (no need to hand‐craft input transformations) but also often improves sample efficiency and convergence, since the model is guided to focus on one meaningful augmentation per variable.

2.  Structured, interpretable representations and better inductive bias. Because each augmented feature is tied back to a single original input, you can inspect and interpret exactly how the network is transforming each variable. That structural constraint also acts as an inductive bias—rather than letting a large, unstructured hidden layer freely combine everything, the AFnetwork builds in a sensible two‐stage feature expansion that often generalizes better (especially when data are limited) and keeps the total parameter count more controlled.
